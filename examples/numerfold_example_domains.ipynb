{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Numerfold"
      ],
      "metadata": {
        "id": "xDbDNqBmIPv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvykKVJ35Y1f",
        "outputId": "f3b086d6-e036-47b5-8d1f-984eb080cb1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/whit3rabbit/numerfold.git\n",
            "  Cloning https://github.com/whit3rabbit/numerfold.git to /tmp/pip-req-build-p62pu5hz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/whit3rabbit/numerfold.git /tmp/pip-req-build-p62pu5hz\n",
            "  Resolved https://github.com/whit3rabbit/numerfold.git to commit d0e4dc6f731c2d8fd2856367efa2561505b6c448\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numeraifold\n",
            "  Building wheel for numeraifold (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numeraifold: filename=numeraifold-0.1.0-py3-none-any.whl size=75947 sha256=25f8b5142f4c79506b3cd380a6b0dbd2ed6da5f0548b1d03491ed41d7ec94a33\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lrdvoxql/wheels/76/85/16/02855ae63bf34628b8629803efc02db199c874582c7fa35ebc\n",
            "Successfully built numeraifold\n",
            "Installing collected packages: numeraifold\n",
            "Successfully installed numeraifold-0.1.0\n",
            "Collecting numerapi\n",
            "  Downloading numerapi-2.20.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting numerblox\n",
            "  Downloading numerblox-1.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Collecting lightgbm\n",
            "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting feature-engine\n",
            "  Downloading feature_engine-1.8.3-py2.py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (17.0.0)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting numerai-tools\n",
            "  Downloading numerai_tools-0.2.3-py3-none-any.whl.metadata (1000 bytes)\n",
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.11/dist-packages (2024.10.0)\n",
            "Collecting dask[dataframe]\n",
            "  Downloading dask-2025.2.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from numerapi) (2.32.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from numerapi) (2025.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from numerapi) (2.8.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from numerapi) (8.1.8)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from numerapi) (2.2.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.3 in /usr/local/lib/python3.11/dist-packages (from numerblox) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from numerblox) (1.13.1)\n",
            "Collecting pandas-ta==0.3.14b (from numerblox)\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from numerblox) (1.4.2)\n",
            "Requirement already satisfied: matplotlib>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from numerblox) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from numerblox) (1.6.1)\n",
            "Requirement already satisfied: google-cloud-storage>=2.11.0 in /usr/local/lib/python3.11/dist-packages (from numerblox) (2.19.0)\n",
            "Collecting numerai-era-data>=0.1.1 (from numerblox)\n",
            "  Downloading numerai_era_data-1.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: polars>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from numerblox) (1.9.0)\n",
            "Requirement already satisfied: werkzeug>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from numerblox) (3.1.3)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.61.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (8.6.1)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from feature-engine) (0.14.4)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=2.11.0->numerblox) (2.27.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=2.11.0->numerblox) (2.24.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=2.11.0->numerblox) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=2.11.0->numerblox) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=2.11.0->numerblox) (1.6.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask[dataframe]) (3.21.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.0->numerblox) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.0->numerblox) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.0->numerblox) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.0->numerblox) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.0->numerblox) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.0->numerblox) (3.2.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (from numerai-era-data>=0.1.1->numerblox) (0.2.54)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0->numerapi) (2025.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->numerapi) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->numerapi) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->numerapi) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->numerapi) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->numerapi) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.1->numerblox) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.11.1->feature-engine) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.3->numerblox) (3.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=2.11.0->numerblox) (1.67.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=2.11.0->numerblox) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=2.11.0->numerblox) (1.26.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage>=2.11.0->numerblox) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage>=2.11.0->numerblox) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage>=2.11.0->numerblox) (4.9)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance->numerai-era-data>=0.1.1->numerblox) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance->numerai-era-data>=0.1.1->numerblox) (4.3.6)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance->numerai-era-data>=0.1.1->numerblox) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance->numerai-era-data>=0.1.1->numerblox) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->numerai-era-data>=0.1.1->numerblox) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance->numerai-era-data>=0.1.1->numerblox) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance->numerai-era-data>=0.1.1->numerblox) (4.12.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage>=2.11.0->numerblox) (0.6.1)\n",
            "Downloading numerapi-2.20.0-py3-none-any.whl (25 kB)\n",
            "Downloading numerblox-1.6.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feature_engine-1.8.3-py2.py3-none-any.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.6/378.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numerai_tools-0.2.3-py3-none-any.whl (10 kB)\n",
            "Downloading numerai_era_data-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask-2025.2.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pandas-ta\n",
            "  Building wheel for pandas-ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218909 sha256=6528c835a1ae555f1dc225fb70ce797581e0b66f4513c95e0334670e9b5e9bf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/33/8b/50b245c5c65433cd8f5cb24ac15d97e5a3db2d41a8b6ae957d\n",
            "Successfully built pandas-ta\n",
            "Installing collected packages: pyarrow, lightgbm, dask, pynndescent, pandas-ta, numerapi, numerai-tools, hdbscan, umap-learn, numerai-era-data, feature-engine, numerblox\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 4.5.0\n",
            "    Uninstalling lightgbm-4.5.0:\n",
            "      Successfully uninstalled lightgbm-4.5.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2024.10.0\n",
            "    Uninstalling dask-2024.10.0:\n",
            "      Successfully uninstalled dask-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.1 which is incompatible.\n",
            "cudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dask-2025.2.0 feature-engine-1.8.3 hdbscan-0.8.40 lightgbm-4.6.0 numerai-era-data-1.0.0 numerai-tools-0.2.3 numerapi-2.20.0 numerblox-1.6.0 pandas-ta-0.3.14b0 pyarrow-19.0.1 pynndescent-0.5.13 umap-learn-0.5.7\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps --ignore-installed git+https://github.com/whit3rabbit/numerfold.git # Dependencies are mostly installed in colab\n",
        "!pip install --upgrade numerapi numerblox hdbscan umap-learn lightgbm seaborn tqdm dask[dataframe] feature-engine pyarrow numerai-tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and constants"
      ],
      "metadata": {
        "id": "ZHSYvJ_7IT2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import traceback\n",
        "import torch\n",
        "import joblib\n",
        "\n",
        "from numeraifold.data.loading import load_data\n",
        "from numeraifold.domains.models import make_predictions_pipeline\n",
        "from numeraifold.pipeline.execution import run_domains_only_pipeline, follow_up_domains_pipeline\n",
        "from numeraifold.utils.seed import set_seed\n",
        "\n",
        "# Optional: Import numerai-tools for scoring if available\n",
        "try:\n",
        "    from numerapi import NumerAPI\n",
        "    from numerai_tools.scoring import correlation_contribution\n",
        "    NUMERAI_TOOLS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMERAI_TOOLS_AVAILABLE = False\n",
        "    print(\"numerai-tools not available. Will use basic scoring methods.\")\n",
        "\n",
        "# Set seed and check CUDA\n",
        "RANDOM_SEED = 42\n",
        "set_seed(RANDOM_SEED)\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeTzbJxF5mjq",
        "outputId": "43260e0d-744a-4a62-d6e4-30f5a6b63f92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Domains-Only Pipeline"
      ],
      "metadata": {
        "id": "fAJCEiSyIMww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Numerai Pipeline: Domain Identification, Model Training, and Scoring ===\n",
        "\n",
        "# --- Phase 1: Domain Identification ---\n",
        "# - Loads medium feature set and converts to float32 for efficiency.\n",
        "# - Uses t-SNE and silhouette scores to cluster features into 28 domains.\n",
        "# - Evaluates feature-target correlations and selects top domains for modeling.\n",
        "\n",
        "# --- Phase 2: Model Training ---\n",
        "# - Prunes highly correlated features (threshold = 0.95).\n",
        "# - Trains LightGBM models for top domains.\n",
        "# - Constructs an ensemble model and a global model, comparing their performance.\n",
        "\n",
        "# --- Phase 3: Model Validation & Scoring ---\n",
        "# - Loads trained models and applies them to validation data.\n",
        "# - Handles potential feature mismatches during validation.\n",
        "# - Computes correlation scores and Sharpe ratios for evaluation.\n",
        "# - Uses Numerai-tools for advanced correlation analysis (if available).\n",
        "\n",
        "# --- Key Findings ---\n",
        "# - Low feature-target correlation is expected in Numerai; even small correlations (~0.01) are significant.\n",
        "# - The global model (0.0113 correlation) outperformed the ensemble (0.0106 correlation).\n",
        "# - Feature mismatches in validation require careful feature consistency handling.\n",
        "\n",
        "# Pipeline successfully processes Numerai data while handling obfuscation and high target correlation.\n",
        "\n",
        "# Medium features: 25 GB RAM\n",
        "# All features: Crashes (50+)"
      ],
      "metadata": {
        "id": "0cmfpW8-vZHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_memory_usage(label=\"Current memory usage\"):\n",
        "    \"\"\"Log the current memory usage of the process.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "    memory_usage_mb = memory_info.rss / (1024 * 1024)\n",
        "\n",
        "    # Get GPU memory usage if available\n",
        "    gpu_memory_usage = \"N/A\"\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            gpu_memory_allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
        "            gpu_memory_reserved = torch.cuda.memory_reserved() / (1024 * 1024)\n",
        "            gpu_memory_usage = f\"Allocated: {gpu_memory_allocated:.2f} MB, Reserved: {gpu_memory_reserved:.2f} MB\"\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(f\"{label} - RAM: {memory_usage_mb:.2f} MB, GPU: {gpu_memory_usage}\")\n",
        "\n",
        "def load_domain_models(models_dir='domain_models'):\n",
        "    \"\"\"\n",
        "    Load all saved domain models and their metadata\n",
        "\n",
        "    Args:\n",
        "        models_dir: Directory where domain models are saved\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of domain models and their metadata\n",
        "    \"\"\"\n",
        "    if not os.path.exists(models_dir):\n",
        "        print(f\"Error: Models directory {models_dir} does not exist\")\n",
        "        return {}\n",
        "\n",
        "    domain_models = {}\n",
        "\n",
        "    # Load ensemble weights if available\n",
        "    ensemble_weights = {}\n",
        "    ensemble_weights_path = os.path.join(models_dir, 'ensemble_weights.json')\n",
        "    if os.path.exists(ensemble_weights_path):\n",
        "        try:\n",
        "            with open(ensemble_weights_path, 'r') as f:\n",
        "                ensemble_data = json.load(f)\n",
        "                ensemble_weights = ensemble_data.get('domains', {})\n",
        "                print(f\"Loaded ensemble weights for {len(ensemble_weights)} domains\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading ensemble weights: {e}\")\n",
        "\n",
        "    # Find all domain model files\n",
        "    model_files = [f for f in os.listdir(models_dir) if f.endswith('_model.joblib')]\n",
        "\n",
        "    for model_file in model_files:\n",
        "        try:\n",
        "            # Extract domain name from filename\n",
        "            domain = model_file.replace('_model.joblib', '')\n",
        "\n",
        "            # Load model\n",
        "            model_path = os.path.join(models_dir, model_file)\n",
        "            model = joblib.load(model_path)\n",
        "\n",
        "            # Load metadata\n",
        "            metadata_path = os.path.join(models_dir, f\"{domain}_metadata.json\")\n",
        "            if os.path.exists(metadata_path):\n",
        "                with open(metadata_path, 'r') as f:\n",
        "                    metadata = json.load(f)\n",
        "            else:\n",
        "                print(f\"Warning: No metadata found for {domain}, using defaults\")\n",
        "                metadata = {\n",
        "                    'domain': domain,\n",
        "                    'features': [],\n",
        "                    'score': 0.0,\n",
        "                    'weight': ensemble_weights.get(domain, 0.0)\n",
        "                }\n",
        "\n",
        "            # Add model to dictionary\n",
        "            domain_models[domain] = {\n",
        "                'model': model,\n",
        "                'features': metadata['features'],\n",
        "                'score': metadata['score'],\n",
        "                'weight': metadata.get('weight', ensemble_weights.get(domain, 0.0))\n",
        "            }\n",
        "            print(f\"Loaded model for {domain} with {len(metadata['features'])} features\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {model_file}: {e}\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(domain_models)} domain models\")\n",
        "    return domain_models\n",
        "\n",
        "def predict_with_domain_models(df, domain_models, target_col=None):\n",
        "    \"\"\"\n",
        "    Make predictions using domain models\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing features\n",
        "        domain_models: Dictionary of domain models from load_domain_models()\n",
        "        target_col: Optional target column for evaluation\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing predictions and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Initialize results\n",
        "    results = {\n",
        "        'domain_predictions': {},\n",
        "        'ensemble_prediction': None,\n",
        "        'domain_scores': {},\n",
        "        'best_domain': None,\n",
        "        'best_score': 0.0\n",
        "    }\n",
        "\n",
        "    if len(domain_models) == 0:\n",
        "        print(\"No domain models available for prediction\")\n",
        "        return results\n",
        "\n",
        "    # For ensemble prediction\n",
        "    ensemble_pred = None\n",
        "    total_weight = 0.0\n",
        "\n",
        "    # Make predictions with each domain model\n",
        "    for domain, model_info in domain_models.items():\n",
        "        model = model_info['model']\n",
        "        features = model_info['features']\n",
        "        weight = model_info['weight']\n",
        "\n",
        "        # Check if all features are available\n",
        "        missing_features = [f for f in features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            print(f\"Warning: {len(missing_features)} features missing for {domain}\")\n",
        "            if len(missing_features) / len(features) > 0.1:  # If more than 10% missing\n",
        "                print(f\"Skipping {domain} due to too many missing features\")\n",
        "                continue\n",
        "\n",
        "        # Prepare features\n",
        "        valid_features = [f for f in features if f in df.columns]\n",
        "        if len(valid_features) < 5:\n",
        "            print(f\"Not enough valid features for {domain}, skipping\")\n",
        "            continue\n",
        "\n",
        "        X = df[valid_features].fillna(0).astype(np.float32)\n",
        "\n",
        "        # Make predictions\n",
        "        try:\n",
        "            domain_pred = model.predict(X)\n",
        "            results['domain_predictions'][domain] = domain_pred\n",
        "\n",
        "            # Add to ensemble if weight > 0\n",
        "            if weight > 0:\n",
        "                if ensemble_pred is None:\n",
        "                    ensemble_pred = np.zeros_like(domain_pred)\n",
        "                ensemble_pred += weight * domain_pred\n",
        "                total_weight += weight\n",
        "\n",
        "            # Evaluate if target column provided\n",
        "            if target_col is not None and target_col in df.columns:\n",
        "                y_true = df[target_col].fillna(0).astype(np.float32).values\n",
        "\n",
        "                # Calculate correlation\n",
        "                mask = ~np.isnan(domain_pred) & ~np.isnan(y_true)\n",
        "                if mask.sum() >= 10:  # Need at least 10 valid pairs\n",
        "                    corr = np.corrcoef(domain_pred[mask], y_true[mask])[0, 1]\n",
        "                    results['domain_scores'][domain] = float(corr)\n",
        "\n",
        "                    # Track best domain\n",
        "                    if corr > results['best_score']:\n",
        "                        results['best_domain'] = domain\n",
        "                        results['best_score'] = float(corr)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error making predictions with {domain}: {e}\")\n",
        "\n",
        "    # Normalize ensemble prediction\n",
        "    if ensemble_pred is not None and total_weight > 0:\n",
        "        ensemble_pred = ensemble_pred / total_weight\n",
        "        results['ensemble_prediction'] = ensemble_pred\n",
        "\n",
        "        # Evaluate ensemble if target provided\n",
        "        if target_col is not None and target_col in df.columns:\n",
        "            y_true = df[target_col].fillna(0).astype(np.float32).values\n",
        "\n",
        "            # Calculate correlation\n",
        "            mask = ~np.isnan(ensemble_pred) & ~np.isnan(y_true)\n",
        "            if mask.sum() >= 10:\n",
        "                corr = np.corrcoef(ensemble_pred[mask], y_true[mask])[0, 1]\n",
        "                results['ensemble_score'] = float(corr)\n",
        "\n",
        "                # Check if ensemble is best\n",
        "                if corr > results['best_score']:\n",
        "                    results['best_model'] = 'ensemble'\n",
        "                    results['best_score'] = float(corr)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Made predictions with {len(results['domain_predictions'])} domain models\")\n",
        "    if 'ensemble_prediction' in results and results['ensemble_prediction'] is not None:\n",
        "        print(f\"Ensemble prediction created with {total_weight:.4f} total weight\")\n",
        "    if 'domain_scores' in results:\n",
        "        print(f\"Domain performance:\")\n",
        "        for domain, score in sorted(results['domain_scores'].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "            print(f\"  {domain}: {score:.4f}\")\n",
        "        if 'ensemble_score' in results:\n",
        "            print(f\"Ensemble score: {results['ensemble_score']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def make_predictions_pipeline(new_data, target_col=None, models_dir='domain_models'):\n",
        "    \"\"\"\n",
        "    End-to-end pipeline for making predictions on new data\n",
        "\n",
        "    Args:\n",
        "        new_data: DataFrame with features\n",
        "        target_col: Optional target column for evaluation\n",
        "        models_dir: Directory containing saved domain models\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with predictions\n",
        "    \"\"\"\n",
        "    # Load domain models\n",
        "    domain_models = load_domain_models(models_dir)\n",
        "    if not domain_models:\n",
        "        print(\"No domain models found, cannot make predictions\")\n",
        "        return None\n",
        "\n",
        "    # Make predictions\n",
        "    results = predict_with_domain_models(new_data, domain_models, target_col)\n",
        "\n",
        "    # Create output DataFrame - keep the original index\n",
        "    output_df = pd.DataFrame(index=new_data.index)\n",
        "\n",
        "    # Add individual domain predictions\n",
        "    for domain, preds in results['domain_predictions'].items():\n",
        "        output_df[f'pred_{domain}'] = preds\n",
        "\n",
        "    # Add ensemble prediction\n",
        "    if results['ensemble_prediction'] is not None:\n",
        "        output_df['prediction_ensemble'] = results['ensemble_prediction']\n",
        "\n",
        "    # Add best prediction\n",
        "    if 'best_model' in results:\n",
        "        if results['best_model'] == 'ensemble':\n",
        "            output_df['prediction_best'] = results['ensemble_prediction']\n",
        "        else:\n",
        "            output_df['prediction_best'] = results['domain_predictions'][results['best_domain']]\n",
        "    elif 'best_domain' in results and results['best_domain'] is not None:\n",
        "        output_df['prediction_best'] = results['domain_predictions'][results['best_domain']]\n",
        "\n",
        "    # Copy important columns from the input data\n",
        "    # This is key - we need to preserve era and other metadata columns\n",
        "    important_cols = ['era'] if 'era' in new_data.columns else []\n",
        "    if target_col is not None and target_col in new_data.columns:\n",
        "        important_cols.append(target_col)\n",
        "\n",
        "    # Add any ID columns\n",
        "    if 'id' in new_data.columns:\n",
        "        important_cols.append('id')\n",
        "\n",
        "    # Copy these columns to the output DataFrame\n",
        "    for col in important_cols:\n",
        "        output_df[col] = new_data[col]\n",
        "\n",
        "    # Log available columns for debugging\n",
        "    print(f\"Created prediction DataFrame with {len(output_df.columns)} columns\")\n",
        "    print(f\"Columns: {list(output_df.columns)}\")\n",
        "\n",
        "    return output_df\n",
        "\n",
        "def save_domain_models(models_dict, feature_groups, output_dir='domain_models'):\n",
        "    \"\"\"\n",
        "    Save trained domain models to disk\n",
        "\n",
        "    Args:\n",
        "        models_dict: Dictionary of trained models\n",
        "        feature_groups: Dictionary of feature groups for each domain\n",
        "        output_dir: Directory to save models\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "    # Save each model\n",
        "    ensemble_weights = {'domains': {}}\n",
        "\n",
        "    for domain, model_info in models_dict.items():\n",
        "        model = model_info.get('model')\n",
        "        if model is None:\n",
        "            print(f\"Warning: No model found for {domain}, skipping\")\n",
        "            continue\n",
        "\n",
        "        score = model_info.get('score', 0.0)\n",
        "        weight = model_info.get('weight', 0.0)\n",
        "\n",
        "        # Get features for this domain\n",
        "        features = feature_groups.get(domain, [])\n",
        "\n",
        "        # Save model\n",
        "        model_path = os.path.join(output_dir, f\"{domain}_model.joblib\")\n",
        "        try:\n",
        "            joblib.dump(model, model_path)\n",
        "\n",
        "            # Save metadata\n",
        "            metadata = {\n",
        "                'domain': domain,\n",
        "                'features': features,\n",
        "                'score': float(score),\n",
        "                'weight': float(weight),\n",
        "                'saved_at': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            }\n",
        "\n",
        "            metadata_path = os.path.join(output_dir, f\"{domain}_metadata.json\")\n",
        "            with open(metadata_path, 'w') as f:\n",
        "                json.dump(metadata, f, indent=4)\n",
        "\n",
        "            # Add to ensemble weights\n",
        "            ensemble_weights['domains'][domain] = float(weight)\n",
        "\n",
        "            print(f\"Saved model and metadata for {domain}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model for {domain}: {e}\")\n",
        "\n",
        "    # Save ensemble weights\n",
        "    ensemble_path = os.path.join(output_dir, 'ensemble_weights.json')\n",
        "    with open(ensemble_path, 'w') as f:\n",
        "        json.dump(ensemble_weights, f, indent=4)\n",
        "\n",
        "    print(f\"Saved ensemble weights for {len(ensemble_weights['domains'])} domains\")\n",
        "\n",
        "def score_predictions_with_numerai_tools(predictions_df, target_col='target', era_col='era'):\n",
        "    \"\"\"\n",
        "    Score predictions using numerai-tools if available\n",
        "\n",
        "    Args:\n",
        "        predictions_df: DataFrame with predictions and targets\n",
        "        target_col: Target column name\n",
        "        era_col: Era column name\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with scoring results\n",
        "    \"\"\"\n",
        "    if not NUMERAI_TOOLS_AVAILABLE:\n",
        "        print(\"numerai-tools not available, using basic scoring\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # First, check if the era column exists in the DataFrame\n",
        "        if era_col not in predictions_df.columns:\n",
        "            print(f\"Error: '{era_col}' column not found in predictions DataFrame\")\n",
        "            print(f\"Available columns: {list(predictions_df.columns)}\")\n",
        "            return None\n",
        "\n",
        "        # Check if target_col exists\n",
        "        if target_col not in predictions_df.columns:\n",
        "            print(f\"Error: '{target_col}' column not found in predictions DataFrame\")\n",
        "            print(f\"Available columns: {list(predictions_df.columns)}\")\n",
        "            return None\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Get prediction columns (those starting with 'pred_' or 'prediction_')\n",
        "        pred_cols = [col for col in predictions_df.columns if col.startswith('pred_') or col.startswith('prediction_')]\n",
        "\n",
        "        if not pred_cols:\n",
        "            print(\"No prediction columns found in DataFrame\")\n",
        "            return None\n",
        "\n",
        "        # Create a DataFrame with just the prediction columns for correlation_contribution\n",
        "        # THE FIX: correlation_contribution expects a DataFrame for predictions\n",
        "        # not numpy arrays or a single Series\n",
        "        preds_df = predictions_df[pred_cols]\n",
        "\n",
        "        # Get meta-model (use first prediction column as meta-model)\n",
        "        # THE FIX: correlation_contribution expects a pandas Series for meta_model\n",
        "        # not a numpy array\n",
        "        meta_model = predictions_df[pred_cols[0]]\n",
        "\n",
        "        # Get targets\n",
        "        # THE FIX: correlation_contribution expects a pandas Series for live_targets\n",
        "        # not a numpy array\n",
        "        targets = predictions_df[target_col]\n",
        "\n",
        "        # Calculate correlation contribution\n",
        "        try:\n",
        "            # THE FIX: This is the key part that fixes the error in the traceback\n",
        "            # We pass pandas objects (DataFrame, Series, Series) instead of numpy arrays\n",
        "            # The error occurred because numpy arrays don't have a .dropna() method\n",
        "            # which is called by filter_sort_index() inside correlation_contribution()\n",
        "            cc = correlation_contribution(\n",
        "                preds_df,  # Pass DataFrame of predictions (not .values)\n",
        "                meta_model,  # Pass meta-model Series (not .values)\n",
        "                targets  # Pass targets Series (not .values)\n",
        "            )\n",
        "\n",
        "            # Process results for each prediction column\n",
        "            for pred_col in pred_cols:\n",
        "                # Get correlation by era\n",
        "                # Calculate per-era correlation (requires at least 5 data points per era)\n",
        "                pred_by_era = predictions_df.groupby(era_col).apply(\n",
        "                    lambda x: x[pred_col].corr(x[target_col]) if len(x) > 5 else np.nan\n",
        "                ).dropna()\n",
        "\n",
        "                # Calculate metrics\n",
        "                corr_mean = pred_by_era.mean()\n",
        "                corr_std = pred_by_era.std()\n",
        "                sharpe = corr_mean / corr_std if corr_std > 0 else 0\n",
        "\n",
        "                # Store metrics\n",
        "                results[pred_col] = {\n",
        "                    'mean_correlation': corr_mean,\n",
        "                    'std_correlation': corr_std,\n",
        "                    'sharpe_ratio': sharpe,\n",
        "                    'feature_exposure': cc.get(pred_col, 0),\n",
        "                    'correlation_by_era': pred_by_era.to_dict()\n",
        "                }\n",
        "\n",
        "            print(\"Scoring with numerai-tools completed\")\n",
        "            return results\n",
        "\n",
        "        except Exception as inner_e:\n",
        "            # Catch errors specifically in the correlation_contribution calculation\n",
        "            print(f\"Error in correlation_contribution calculation: {inner_e}\")\n",
        "            traceback.print_exc()\n",
        "            raise inner_e\n",
        "\n",
        "    except Exception as e:\n",
        "        # This catches the error shown in the traceback and prints it\n",
        "        print(f\"Error in scoring with numerai-tools: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Fallback to basic correlation scoring\n",
        "        print(\"Falling back to basic correlation scoring...\")\n",
        "        return None\n",
        "\n",
        "def run_integrated_pipeline(data_version=\"v5.0\",\n",
        "                            feature_set=\"medium\",\n",
        "                            sample_size=100000,\n",
        "                            domain_score_threshold=0.05,\n",
        "                            correlation_threshold=0.95,\n",
        "                            tournament_data_path=None,\n",
        "                            models_dir='domain_models'):\n",
        "    \"\"\"\n",
        "    Run the integrated pipeline from domain identification to predictions\n",
        "\n",
        "    Args:\n",
        "        data_version: Numerai data version\n",
        "        feature_set: Feature set to use (small, medium, all)\n",
        "        sample_size: Number of samples to use\n",
        "        domain_score_threshold: Min threshold for domain scores\n",
        "        correlation_threshold: Threshold for feature correlation pruning\n",
        "        tournament_data_path: Path to tournament data file for predictions\n",
        "        models_dir: Directory to save/load models\n",
        "\n",
        "    Returns:\n",
        "        dict: Results from the pipeline\n",
        "    \"\"\"\n",
        "    # Start with clean memory\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    start_time = time.time()\n",
        "    log_memory_usage(\"Starting pipeline\")\n",
        "\n",
        "    results = {\n",
        "        'domains_identified': 0,\n",
        "        'models_trained': 0,\n",
        "        'models_saved': True,\n",
        "        'validation_score': None,\n",
        "        'tournament_predictions': None\n",
        "    }\n",
        "\n",
        "    # Step 1: Run domains pipeline to identify feature groups\n",
        "    print(\"\\n=== Phase 1: Domain Identification ===\")\n",
        "    domains_results = run_domains_only_pipeline(\n",
        "        data_version=data_version,\n",
        "        feature_set=feature_set,\n",
        "        main_target=\"target\",\n",
        "        aux_targets=[\"target_cyrusd_20\", \"target_cyrusd_60\", \"target_teager2b_20\", \"target_teager2b_60\"],\n",
        "        sample_size=sample_size,\n",
        "        n_clusters=None,  # Auto-determine optimal clusters\n",
        "        use_incremental=True,\n",
        "        skip_visualizations=True,\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    if not domains_results or 'feature_groups' not in domains_results:\n",
        "        print(\"Domain identification failed\")\n",
        "        return results\n",
        "\n",
        "    feature_groups = domains_results['feature_groups']\n",
        "    results['domains_identified'] = len(feature_groups)\n",
        "    print(f\"Identified {results['domains_identified']} domains\")\n",
        "\n",
        "    # Log memory usage\n",
        "    log_memory_usage(\"After domain identification\")\n",
        "\n",
        "    # Step 2: Run follow-up pipeline to train models\n",
        "    print(\"\\n=== Phase 2: Model Training ===\")\n",
        "\n",
        "    # Clean memory before loading data\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Load data for training\n",
        "    train_df, val_df, features, all_targets = load_data(\n",
        "        data_version=data_version,\n",
        "        feature_set=feature_set,\n",
        "        main_target=\"target\",\n",
        "        num_aux_targets=3,\n",
        "        sample_size=sample_size,\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    if train_df is None or val_df is None:\n",
        "        print(\"Failed to load training/validation data\")\n",
        "        return results\n",
        "\n",
        "    log_memory_usage(\"After data loading\")\n",
        "\n",
        "    # Run follow-up pipeline\n",
        "    followup_results = follow_up_domains_pipeline(\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        feature_groups=feature_groups,\n",
        "        main_target=\"target\",\n",
        "        domain_score_threshold=domain_score_threshold,\n",
        "        correlation_threshold=correlation_threshold\n",
        "    )\n",
        "\n",
        "    if not followup_results or 'domain_models' not in followup_results:\n",
        "        print(\"Model training failed\")\n",
        "        return results\n",
        "\n",
        "    domain_models = followup_results['domain_models']\n",
        "    results['models_trained'] = len(domain_models)\n",
        "    print(f\"Trained {results['models_trained']} domain models\")\n",
        "\n",
        "    # Save the final score\n",
        "    if 'final_model_score' in followup_results:\n",
        "        results['validation_score'] = followup_results['final_model_score']\n",
        "        print(f\"Validation score: {results['validation_score']:.4f}\")\n",
        "\n",
        "    # Step 3: Save models\n",
        "    print(\"\\n=== Phase 3: Saving Models ===\")\n",
        "    try:\n",
        "        save_domain_models(\n",
        "            models_dict=domain_models,\n",
        "            feature_groups=feature_groups,\n",
        "            output_dir=models_dir\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving models: {e}\")\n",
        "        results['models_saved'] = False\n",
        "\n",
        "    # Step 4: Make tournament predictions if path provided\n",
        "    if tournament_data_path:\n",
        "        print(\"\\n=== Phase 4: Tournament Predictions ===\")\n",
        "        try:\n",
        "            # Clean memory\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Load tournament data\n",
        "            print(f\"Loading tournament data from {tournament_data_path}\")\n",
        "            tournament_data = pd.read_parquet(tournament_data_path)\n",
        "            print(f\"Loaded tournament data with shape {tournament_data.shape}\")\n",
        "\n",
        "            # Convert features to float32\n",
        "            for col in tournament_data.columns:\n",
        "                if col in features:\n",
        "                    tournament_data[col] = tournament_data[col].astype(np.float32)\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = make_predictions_pipeline(\n",
        "                tournament_data,\n",
        "                models_dir=models_dir\n",
        "            )\n",
        "\n",
        "            if predictions is not None:\n",
        "                # Format for submission\n",
        "                if 'prediction_ensemble' in predictions.columns:\n",
        "                    predictions['prediction'] = predictions['prediction_ensemble']\n",
        "                elif 'prediction_best' in predictions.columns:\n",
        "                    predictions['prediction'] = predictions['prediction_best']\n",
        "\n",
        "                # Keep only required columns\n",
        "                id_cols = ['id', 'era'] if 'id' in tournament_data.columns else ['era']\n",
        "                submission = predictions[id_cols + ['prediction']]\n",
        "\n",
        "                # Save submission\n",
        "                submission_path = 'numerai_submission.csv'\n",
        "                submission.to_csv(submission_path, index=False)\n",
        "\n",
        "                print(f\"Saved tournament predictions to {submission_path}\")\n",
        "                results['tournament_predictions'] = submission_path\n",
        "            else:\n",
        "                print(\"Failed to generate predictions\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error making tournament predictions: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # Calculate total runtime\n",
        "    total_time = (time.time() - start_time) / 60  # minutes\n",
        "    print(f\"\\nPipeline completed in {total_time:.2f} minutes\")\n",
        "\n",
        "    # Log final memory usage\n",
        "    log_memory_usage(\"End of pipeline\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def score_models_with_validation(models_dir='domain_models', data_version=\"v5.0\",\n",
        "                                feature_set=\"medium\", sample_size=100000):\n",
        "    \"\"\"\n",
        "    Score saved models using validation data\n",
        "\n",
        "    Args:\n",
        "        models_dir: Directory containing saved models\n",
        "        data_version: Numerai data version\n",
        "        feature_set: Feature set to use\n",
        "        sample_size: Sample size for validation\n",
        "\n",
        "    Returns:\n",
        "        dict: Scoring results\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Scoring Models with Validation Data ===\")\n",
        "\n",
        "    # Load validation data\n",
        "    train_df, val_df, _, _ = load_data(\n",
        "        data_version=data_version,\n",
        "        feature_set=feature_set,\n",
        "        main_target=\"target\",\n",
        "        num_aux_targets=3,\n",
        "        sample_size=sample_size * 2,  # Use more data for validation\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    if val_df is None:\n",
        "        print(\"Failed to load validation data\")\n",
        "        return None\n",
        "\n",
        "    # Verify 'era' column exists\n",
        "    if 'era' not in val_df.columns:\n",
        "        print(\"Warning: 'era' column not found in validation data\")\n",
        "        print(f\"Available columns: {list(val_df.columns)}\")\n",
        "        print(\"Adding dummy 'era' column for compatibility\")\n",
        "        # Add a dummy era column if needed (e.g., all rows in one era)\n",
        "        val_df['era'] = 1\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = make_predictions_pipeline(\n",
        "        val_df,\n",
        "        target_col=\"target\",\n",
        "        models_dir=models_dir\n",
        "    )\n",
        "\n",
        "    if predictions is None:\n",
        "        print(\"Failed to generate predictions\")\n",
        "        return None\n",
        "\n",
        "    # Verify columns in predictions DataFrame\n",
        "    print(f\"Prediction columns: {list(predictions.columns)}\")\n",
        "\n",
        "    # Ensure target column is present\n",
        "    if 'target' not in predictions.columns and 'target' in val_df.columns:\n",
        "        print(\"Adding target column to predictions DataFrame\")\n",
        "        predictions['target'] = val_df['target']\n",
        "\n",
        "    # Make sure era column is present for proper scoring\n",
        "    if 'era' not in predictions.columns:\n",
        "        print(\"Warning: 'era' column missing from predictions, adding from validation data\")\n",
        "        if 'era' in val_df.columns:\n",
        "            predictions['era'] = val_df['era']\n",
        "        else:\n",
        "            print(\"Creating a dummy 'era' column (all rows in one era)\")\n",
        "            predictions['era'] = 1\n",
        "\n",
        "    # Score with numerai-tools if available\n",
        "    if NUMERAI_TOOLS_AVAILABLE:\n",
        "        print(\"Scoring with numerai-tools...\")\n",
        "        try:\n",
        "            # Check that we have both the target and era columns\n",
        "            if 'target' not in predictions.columns:\n",
        "                print(\"Error: 'target' column not found in predictions DataFrame\")\n",
        "                raise ValueError(\"Missing target column\")\n",
        "\n",
        "            if 'era' not in predictions.columns:\n",
        "                print(\"Error: 'era' column not found in predictions DataFrame\")\n",
        "                raise ValueError(\"Missing era column\")\n",
        "\n",
        "            # Get prediction columns\n",
        "            pred_cols = [col for col in predictions.columns\n",
        "                        if col.startswith('pred_') or col.startswith('prediction_')]\n",
        "\n",
        "            if not pred_cols:\n",
        "                print(\"Error: No prediction columns found\")\n",
        "                raise ValueError(\"No prediction columns\")\n",
        "\n",
        "            # Use our fixed scoring function\n",
        "            scoring_results = score_predictions_with_numerai_tools(\n",
        "                predictions_df=predictions,\n",
        "                target_col=\"target\",\n",
        "                era_col=\"era\"\n",
        "            )\n",
        "\n",
        "            if scoring_results:\n",
        "                # Save results to JSON\n",
        "                scores_path = os.path.join(models_dir, 'validation_scores.json')\n",
        "                with open(scores_path, 'w') as f:\n",
        "                    # Convert numpy types to Python native types for JSON\n",
        "                    json_results = {}\n",
        "                    for key, value in scoring_results.items():\n",
        "                        # Skip correlation_by_era which can be large and may contain non-serializable types\n",
        "                        json_results[key] = {\n",
        "                            k: float(v) if isinstance(v, (np.float32, np.float64)) else v\n",
        "                            for k, v in value.items() if k != 'correlation_by_era'\n",
        "                        }\n",
        "\n",
        "                    json.dump(json_results, f, indent=4)\n",
        "\n",
        "                print(f\"Saved validation scores to {scores_path}\")\n",
        "                return scoring_results\n",
        "        except Exception as e:\n",
        "            print(f\"Error in numerai-tools scoring: {e}\")\n",
        "            traceback.print_exc()\n",
        "            print(\"Falling back to basic correlation scoring...\")\n",
        "    else:\n",
        "        print(\"numerai-tools not available, using basic correlation scoring\")\n",
        "\n",
        "    # Basic correlation scoring as fallback\n",
        "    print(\"Using basic correlation scoring...\")\n",
        "\n",
        "    # Check if 'era' exists for grouping\n",
        "    if 'era' in predictions.columns:\n",
        "        eras = predictions['era'].unique()\n",
        "    else:\n",
        "        # Create a single era if 'era' column is missing\n",
        "        eras = [1]\n",
        "        predictions['era'] = 1\n",
        "\n",
        "    # Calculate metrics per era\n",
        "    results = []\n",
        "    for era in eras:\n",
        "        era_data = predictions[predictions['era'] == era]\n",
        "\n",
        "        if len(era_data) == 0:\n",
        "            continue\n",
        "\n",
        "        era_result = {'era': era}\n",
        "\n",
        "        # Calculate metrics for each prediction column\n",
        "        for col in era_data.columns:\n",
        "            if col.startswith('pred_') or col.startswith('prediction_'):\n",
        "                # Calculate correlation with target\n",
        "                if 'target' in era_data.columns:\n",
        "                    # Use pandas correlation method to avoid potential issues with np.corrcoef\n",
        "                    try:\n",
        "                        # Remove NaN values\n",
        "                        valid_mask = ~np.isnan(era_data[col]) & ~np.isnan(era_data['target'])\n",
        "                        if valid_mask.sum() >= 10:  # Need at least 10 valid pairs\n",
        "                            corr = era_data[col][valid_mask].corr(era_data['target'][valid_mask])\n",
        "                            era_result[f\"{col}_corr\"] = corr\n",
        "                    except Exception as calc_err:\n",
        "                        print(f\"Error calculating correlation for {col} in era {era}: {calc_err}\")\n",
        "\n",
        "        results.append(era_result)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary = {}\n",
        "    prediction_cols = [col for col in results_df.columns if col.endswith('_corr')]\n",
        "\n",
        "    for col in prediction_cols:\n",
        "        model_name = col.replace('_corr', '')\n",
        "        # Handle empty or all-NaN columns\n",
        "        if col in results_df.columns and not results_df[col].isna().all():\n",
        "            mean_corr = results_df[col].mean()\n",
        "            std_corr = results_df[col].std()\n",
        "\n",
        "            summary[model_name] = {\n",
        "                'mean_correlation': mean_corr,\n",
        "                'median_correlation': results_df[col].median(),\n",
        "                'std_correlation': std_corr,\n",
        "                'min_correlation': results_df[col].min(),\n",
        "                'max_correlation': results_df[col].max(),\n",
        "                'sharpe_ratio': mean_corr / std_corr if std_corr > 0 else 0,\n",
        "                'positive_eras': (results_df[col] > 0).mean() * 100  # Percentage of positive eras\n",
        "            }\n",
        "\n",
        "    # Save summary to file\n",
        "    summary_df = pd.DataFrame.from_dict(summary, orient='index')\n",
        "    summary_df.to_csv(os.path.join(models_dir, 'backtest_summary.csv'))\n",
        "\n",
        "    # Save detailed era results\n",
        "    results_df.to_csv(os.path.join(models_dir, 'backtest_by_era.csv'), index=False)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nScoring Summary:\")\n",
        "    for model_name, metrics in sorted(summary.items(),\n",
        "                                     key=lambda x: x[1]['mean_correlation'],\n",
        "                                     reverse=True)[:5]:\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  Mean Correlation: {metrics['mean_correlation']:.4f}\")\n",
        "        print(f\"  Sharpe Ratio: {metrics['sharpe_ratio']:.4f}\")\n",
        "\n",
        "    print(f\"Saved basic scoring results to {models_dir}/backtest_summary.csv\")\n",
        "\n",
        "    return {'summary': summary, 'by_era': results_df}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define parameters\n",
        "    DATA_VERSION = \"v5.0\"\n",
        "    FEATURE_SET = \"medium\"  # Options: small, medium, all\n",
        "    SAMPLE_SIZE = 100000    # Adjust based on your memory constraints\n",
        "    DOMAIN_SCORE_THRESHOLD = 0.05\n",
        "    CORRELATION_THRESHOLD = 0.95\n",
        "    MODELS_DIR = \"domain_models\"\n",
        "    TOURNAMENT_PATH = None  # Set to tournament.parquet path if available\n",
        "\n",
        "    # Run the full pipeline\n",
        "    results = run_integrated_pipeline(\n",
        "        data_version=DATA_VERSION,\n",
        "        feature_set=FEATURE_SET,\n",
        "        sample_size=SAMPLE_SIZE,\n",
        "        domain_score_threshold=DOMAIN_SCORE_THRESHOLD,\n",
        "        correlation_threshold=CORRELATION_THRESHOLD,\n",
        "        tournament_data_path=TOURNAMENT_PATH,\n",
        "        models_dir=MODELS_DIR\n",
        "    )\n",
        "\n",
        "    # Score models with validation data\n",
        "    if results['models_saved']:\n",
        "        print(\"\\n=== Scoring Models with Validation Data ===\")\n",
        "        scoring_results = score_models_with_validation(\n",
        "            models_dir=MODELS_DIR,\n",
        "            data_version=DATA_VERSION,\n",
        "            feature_set=FEATURE_SET,\n",
        "            sample_size=min(SAMPLE_SIZE * 2, 200000)  # Use more data for scoring if possible\n",
        "        )\n",
        "\n",
        "        if scoring_results:\n",
        "            print(\"\\nScoring Summary:\")\n",
        "\n",
        "            # If using numerai-tools\n",
        "            if isinstance(scoring_results, dict) and 'summary' not in scoring_results:\n",
        "                # Print top models by correlation\n",
        "                top_models = sorted(scoring_results.items(),\n",
        "                                    key=lambda x: x[1]['mean_correlation'],\n",
        "                                    reverse=True)[:5]\n",
        "\n",
        "                for model, metrics in top_models:\n",
        "                    print(f\"{model}:\")\n",
        "                    print(f\"  Mean Correlation: {metrics['mean_correlation']:.4f}\")\n",
        "                    print(f\"  Sharpe Ratio: {metrics['sharpe_ratio']:.4f}\")\n",
        "\n",
        "            # If using basic scoring\n",
        "            elif isinstance(scoring_results, dict) and 'summary' in scoring_results:\n",
        "                top_models = sorted(scoring_results['summary'].items(),\n",
        "                                   key=lambda x: x[1]['mean_correlation'],\n",
        "                                   reverse=True)[:5]\n",
        "\n",
        "                for model, metrics in top_models:\n",
        "                    print(f\"{model}:\")\n",
        "                    print(f\"  Mean Correlation: {metrics['mean_correlation']:.4f}\")\n",
        "                    print(f\"  Sharpe Ratio: {metrics['sharpe']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC7Tq3yNH8hK",
        "outputId": "bb593b54-4992-46e5-e2c5-88031a1fe13a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pipeline - RAM: 1375.43 MB, GPU: N/A\n",
            "\n",
            "=== Phase 1: Domain Identification ===\n",
            "Starting enhanced domain pipeline with medium feature set\n",
            "Initial - RAM: 1375.43 MB, GPU: N/A\n",
            "Loading data with explicit float32 conversion...\n",
            "Using specified auxiliary targets: ['target_cyrusd_20', 'target_cyrusd_60', 'target_teager2b_20', 'target_teager2b_60']\n",
            "Loading v5.0 data with medium feature set...\n",
            "Downloading train.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "v5.0/train.parquet: 2.37GB [00:37, 63.5MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading validation.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "v5.0/validation.parquet: 3.33GB [00:57, 57.7MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading features.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "v5.0/features.json: 291kB [00:00, 1.81MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using targets: ['target', 'target_cyrusd_20', 'target_cyrusd_60', 'target_teager2b_20', 'target_teager2b_60']\n",
            "Reading train data with 705 features and 5 targets...\n",
            "Reading validation data...\n",
            "Converting features and targets to float32...\n",
            "Train shape: (2746270, 711), Validation shape: (3596488, 711)\n",
            "Memory usage - Train: 7789.05 MB, Val: 10200.46 MB\n",
            "Available targets: ['target_cyrusd_20', 'target_cyrusd_60', 'target_teager2b_20', 'target_teager2b_60']\n",
            "Loaded data with 705 features\n",
            "Using targets: ['target', 'target_cyrusd_20', 'target_cyrusd_60', 'target_teager2b_20', 'target_teager2b_60']\n",
            "Train shape: (2746270, 711), Val shape: (3596488, 711)\n",
            "Converting features and targets to float32...\n",
            "After data type conversion - RAM: 23269.19 MB, GPU: N/A\n",
            "Sampling 100000 rows from training data\n",
            "Computing feature-target correlations with memory optimization...\n",
            "Correlation matrix shape: (705, 5)\n",
            "Applying t-SNE for enhanced feature embeddings...\n",
            "Before t-SNE - RAM: 23492.10 MB, GPU: N/A\n",
            "Feature embeddings shape: (705, 3)\n",
            "After t-SNE - RAM: 23493.52 MB, GPU: N/A\n",
            "Determining optimal number of clusters dynamically using silhouette scores...\n",
            "Silhouette score for k=15: 0.3193\n",
            "Silhouette score for k=16: 0.3164\n",
            "Silhouette score for k=17: 0.3173\n",
            "Silhouette score for k=18: 0.3172\n",
            "Silhouette score for k=19: 0.3245\n",
            "Silhouette score for k=20: 0.3281\n",
            "Silhouette score for k=21: 0.3329\n",
            "Silhouette score for k=22: 0.3398\n",
            "Silhouette score for k=23: 0.3286\n",
            "Silhouette score for k=24: 0.3423\n",
            "Silhouette score for k=25: 0.3315\n",
            "Silhouette score for k=26: 0.3421\n",
            "Silhouette score for k=27: 0.3350\n",
            "Silhouette score for k=28: 0.3443\n",
            "Silhouette score for k=29: 0.3440\n",
            "Optimal number of clusters determined: 28\n",
            "Clustering features into 28 domains...\n",
            "Domain sizes:\n",
            "  domain_18: 22 features\n",
            "  domain_17: 22 features\n",
            "  domain_7: 28 features\n",
            "  domain_11: 30 features\n",
            "  domain_8: 27 features\n",
            "  domain_25: 16 features\n",
            "  domain_14: 17 features\n",
            "  domain_20: 16 features\n",
            "  domain_26: 18 features\n",
            "  domain_1: 31 features\n",
            "  domain_16: 27 features\n",
            "  domain_0: 30 features\n",
            "  domain_2: 15 features\n",
            "  domain_5: 21 features\n",
            "  domain_3: 28 features\n",
            "  domain_4: 21 features\n",
            "  domain_24: 22 features\n",
            "  domain_12: 30 features\n",
            "  domain_9: 31 features\n",
            "  domain_21: 38 features\n",
            "  domain_6: 27 features\n",
            "  domain_22: 20 features\n",
            "  domain_13: 23 features\n",
            "  domain_27: 21 features\n",
            "  domain_10: 39 features\n",
            "  domain_15: 20 features\n",
            "  domain_19: 36 features\n",
            "  domain_23: 29 features\n",
            "Saving domain data...\n",
            "Domain data saved to: feature_domains_data.csv\n",
            "Computing intra-domain correlation matrix...\n",
            "Using memory-efficient domain validation approach\n",
            "Before visualization - RAM: 23495.21 MB, GPU: N/A\n",
            "Final - RAM: 23495.21 MB, GPU: N/A\n",
            "Identified 28 domains\n",
            "After domain identification - RAM: 5512.91 MB, GPU: N/A\n",
            "\n",
            "=== Phase 2: Model Training ===\n",
            "Loading v5.0 data with medium feature set...\n",
            "Using targets: ['target', 'target_xerxes_60', 'target_cyrusd_60', 'target_teager2b_20']\n",
            "Reading train data with 705 features and 4 targets...\n",
            "A sample size of 100000 has been specified\n",
            "Train file has 2746270 total rows\n",
            "Loading a sample of 100000 rows from training data...\n",
            "Reading validation data...\n",
            "Converting features and targets to float32...\n",
            "Train shape: (100000, 710), Validation shape: (3596488, 710)\n",
            "Memory usage - Train: 276.28 MB, Val: 10186.74 MB\n",
            "After data loading - RAM: 16315.29 MB, GPU: N/A\n",
            "Starting follow-up domains pipeline with improved NaN handling...\n",
            "Cleaning data and ensuring proper types...\n",
            "Found 705/705 valid features in training data\n",
            "705/705 features were already float32\n",
            "Evaluating domain performance with robust correlation calculation...\n",
            "  domain_18: 0.0009 (from 22 valid features)\n",
            "  domain_17: 0.0027 (from 22 valid features)\n",
            "  domain_7: 0.0043 (from 28 valid features)\n",
            "  domain_11: 0.0087 (from 30 valid features)\n",
            "  domain_8: 0.0047 (from 27 valid features)\n",
            "  domain_25: 0.0080 (from 16 valid features)\n",
            "  domain_14: 0.0008 (from 17 valid features)\n",
            "  domain_20: 0.0042 (from 16 valid features)\n",
            "  domain_26: 0.0022 (from 18 valid features)\n",
            "  domain_1: 0.0039 (from 31 valid features)\n",
            "  domain_16: 0.0052 (from 27 valid features)\n",
            "  domain_0: 0.0028 (from 30 valid features)\n",
            "  domain_2: 0.0011 (from 15 valid features)\n",
            "  domain_5: 0.0062 (from 21 valid features)\n",
            "  domain_3: 0.0025 (from 28 valid features)\n",
            "  domain_4: 0.0121 (from 21 valid features)\n",
            "  domain_24: 0.0012 (from 22 valid features)\n",
            "  domain_12: 0.0027 (from 30 valid features)\n",
            "  domain_9: 0.0028 (from 31 valid features)\n",
            "  domain_21: 0.0046 (from 38 valid features)\n",
            "  domain_6: 0.0006 (from 27 valid features)\n",
            "  domain_22: 0.0048 (from 20 valid features)\n",
            "  domain_13: 0.0029 (from 23 valid features)\n",
            "  domain_27: 0.0034 (from 21 valid features)\n",
            "  domain_10: 0.0012 (from 39 valid features)\n",
            "  domain_15: 0.0009 (from 20 valid features)\n",
            "  domain_19: 0.0075 (from 36 valid features)\n",
            "  domain_23: 0.0017 (from 29 valid features)\n",
            "Domain performance scores (correlation with target):\n",
            "  domain_4: 0.0121\n",
            "  domain_11: 0.0087\n",
            "  domain_25: 0.0080\n",
            "  domain_19: 0.0075\n",
            "  domain_5: 0.0062\n",
            "  domain_16: 0.0052\n",
            "  domain_22: 0.0048\n",
            "  domain_8: 0.0047\n",
            "  domain_21: 0.0046\n",
            "  domain_7: 0.0043\n",
            "  domain_20: 0.0042\n",
            "  domain_1: 0.0039\n",
            "  domain_27: 0.0034\n",
            "  domain_13: 0.0029\n",
            "  domain_9: 0.0028\n",
            "  domain_0: 0.0028\n",
            "  domain_17: 0.0027\n",
            "  domain_12: 0.0027\n",
            "  domain_3: 0.0025\n",
            "  domain_26: 0.0022\n",
            "  domain_23: 0.0017\n",
            "  domain_24: 0.0012\n",
            "  domain_10: 0.0012\n",
            "  domain_2: 0.0011\n",
            "  domain_18: 0.0009\n",
            "  domain_15: 0.0009\n",
            "  domain_14: 0.0008\n",
            "  domain_6: 0.0006\n",
            "\n",
            "Filtering domains with correlation >= 0.05...\n",
            "Kept domains: 0/28\n",
            "Warning: No domains met threshold. Keeping top 3 domains instead.\n",
            "Top domains selected: ['domain_4', 'domain_11', 'domain_25']\n",
            "Total features from kept domains: 67/67 valid features\n",
            "\n",
            "Pruning correlated features (threshold=0.95)...\n",
            "Using robust correlation pruning approach...\n",
            "Calculating feature-target correlations...\n",
            "Pruning 67 features...\n",
            "Pruning complete. Kept 63 features out of 67\n",
            "\n",
            "Training domain-specific models...\n",
            "Pruning features for domain domain_4...\n",
            "Training model for domain domain_4 with 20 features\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003597 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 100\n",
            "[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.499538\n",
            "Domain domain_4 model score: 0.0087, weight: 0.0087\n",
            "Saved model and metadata for domain domain_4\n",
            "Pruning features for domain domain_11...\n",
            "Training model for domain domain_11 with 27 features\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002360 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 135\n",
            "[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.499538\n",
            "Domain domain_11 model score: 0.0073, weight: 0.0073\n",
            "Saved model and metadata for domain domain_11\n",
            "Pruning features for domain domain_25...\n",
            "Training model for domain domain_25 with 16 features\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008354 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 80\n",
            "[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 0.499538\n",
            "Domain domain_25 model score: 0.0029, weight: 0.0029\n",
            "Saved model and metadata for domain domain_25\n",
            "\n",
            "Ensemble model performance:\n",
            "Ensemble correlation score: 0.0106\n",
            "Ensemble Spearman correlation: 0.0104\n",
            "Saved ensemble weights\n",
            "\n",
            "Training final global regression model with pruned features...\n",
            "Training final model on 100000 samples with 63 features...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029489 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 315\n",
            "[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 63\n",
            "[LightGBM] [Info] Start training from score 0.499538\n",
            "Saved final model and metadata\n",
            "Final model correlation score: 0.0113\n",
            "Spearman correlation: 0.0103\n",
            "Global model outperforms ensemble: 0.0113 vs 0.0106\n",
            "Trained 3 domain models\n",
            "Validation score: 0.0113\n",
            "\n",
            "=== Phase 3: Saving Models ===\n",
            "Saved model and metadata for domain_4\n",
            "Saved model and metadata for domain_11\n",
            "Saved model and metadata for domain_25\n",
            "Saved ensemble weights for 3 domains\n",
            "\n",
            "Pipeline completed in 5.81 minutes\n",
            "End of pipeline - RAM: 16528.29 MB, GPU: N/A\n",
            "\n",
            "=== Scoring Models with Validation Data ===\n",
            "\n",
            "=== Scoring Models with Validation Data ===\n",
            "Loading v5.0 data with medium feature set...\n",
            "Using targets: ['target', 'target_delta_20', 'target_xerxes_20', 'target_charlie_60']\n",
            "Reading train data with 705 features and 4 targets...\n",
            "A sample size of 400000 has been specified\n",
            "Train file has 2746270 total rows\n",
            "Loading a sample of 400000 rows from training data...\n",
            "Reading validation data...\n",
            "Converting features and targets to float32...\n",
            "Train shape: (400000, 710), Validation shape: (3596488, 710)\n",
            "Memory usage - Train: 1105.12 MB, Val: 10186.74 MB\n",
            "Loaded ensemble weights for 3 domains\n",
            "Loaded model for domain_11 with 30 features\n",
            "Loaded model for domain_4 with 21 features\n",
            "Loaded model for domain_25 with 16 features\n",
            "Successfully loaded 3 domain models\n",
            "Error making predictions with domain_11: The number of features in data (30) is not the same as it was in training data (27).\n",
            "You can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n",
            "Error making predictions with domain_4: The number of features in data (21) is not the same as it was in training data (20).\n",
            "You can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n",
            "Made predictions with 1 domain models\n",
            "Ensemble prediction created with 0.0029 total weight\n",
            "Domain performance:\n",
            "  domain_25: 0.0028\n",
            "Ensemble score: 0.0028\n",
            "Created prediction DataFrame with 5 columns\n",
            "Columns: ['pred_domain_25', 'prediction_ensemble', 'prediction_best', 'era', 'target']\n",
            "Prediction columns: ['pred_domain_25', 'prediction_ensemble', 'prediction_best', 'era', 'target']\n",
            "Scoring with numerai-tools...\n",
            "Scoring with numerai-tools completed\n",
            "Saved validation scores to domain_models/validation_scores.json\n",
            "\n",
            "Scoring Summary:\n",
            "prediction_ensemble:\n",
            "  Mean Correlation: 0.0027\n",
            "  Sharpe Ratio: 0.2170\n",
            "pred_domain_25:\n",
            "  Mean Correlation: 0.0027\n",
            "  Sharpe Ratio: 0.2170\n",
            "prediction_best:\n",
            "  Mean Correlation: 0.0027\n",
            "  Sharpe Ratio: 0.2170\n"
          ]
        }
      ]
    }
  ]
}