{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Numerfold"
      ],
      "metadata": {
        "id": "xDbDNqBmIPv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvykKVJ35Y1f",
        "outputId": "61994922-f532-496f-84d6-aed9981b8de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/whit3rabbit/numerfold.git\n",
            "  Cloning https://github.com/whit3rabbit/numerfold.git to /tmp/pip-req-build-4mjpvt9x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/whit3rabbit/numerfold.git /tmp/pip-req-build-4mjpvt9x\n",
            "  Resolved https://github.com/whit3rabbit/numerfold.git to commit 722455e3ac543cef3f5fc176954833377b624c76\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numeraifold\n",
            "  Building wheel for numeraifold (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numeraifold: filename=numeraifold-0.1.0-py3-none-any.whl size=76237 sha256=79e5db0ff2cbb3f1fa0c200142249c0964acad2dbb1688e566ea9ad804774eba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-71iph2yl/wheels/76/85/16/02855ae63bf34628b8629803efc02db199c874582c7fa35ebc\n",
            "Successfully built numeraifold\n",
            "Installing collected packages: numeraifold\n",
            "Successfully installed numeraifold-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps --ignore-installed git+https://github.com/whit3rabbit/numerfold.git # Dependencies are mostly installed in colab\n",
        "!pip install --upgrade numerapi numerblox hdbscan umap-learn lightgbm seaborn tqdm dask[dataframe] feature-engine pyarrow numerai-tools -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and constants"
      ],
      "metadata": {
        "id": "ZHSYvJ_7IT2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import traceback\n",
        "import torch\n",
        "import joblib\n",
        "\n",
        "from numeraifold.data.loading import load_data\n",
        "from numeraifold.domains.models import make_predictions_pipeline\n",
        "from numeraifold.pipeline.execution import run_domains_only_pipeline, follow_up_domains_pipeline\n",
        "from numeraifold.utils.seed import set_seed\n",
        "\n",
        "# Optional: Import numerai-tools for scoring if available\n",
        "try:\n",
        "    from numerapi import NumerAPI\n",
        "    from numerai_tools.scoring import correlation_contribution\n",
        "    NUMERAI_TOOLS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMERAI_TOOLS_AVAILABLE = False\n",
        "    print(\"numerai-tools not available. Will use basic scoring methods.\")"
      ],
      "metadata": {
        "id": "IeTzbJxF5mjq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global constants\n",
        "RANDOM_SEED = 42\n",
        "DATA_VERSION = \"v5.0\"  # Default Numerai data version\n",
        "DEFAULT_FEATURE_SET = \"medium\"  # Options: small, medium, all\n",
        "DEFAULT_SAMPLE_SIZE = 100000  # Adjust based on memory constraints\n",
        "\n",
        "# File locations as constants\n",
        "OUTPUT_DIR = \"numeraifold_results\"  # Main output directory\n",
        "MODELS_DIR = \"domain_models\"  # Models directory\n",
        "ALPHAFOLD_DIR = os.path.join(OUTPUT_DIR, \"alphafold\")  # AlphaFold results directory\n",
        "TOURNAMENT_DATA_DEFAULT = None  # Default path to tournament data"
      ],
      "metadata": {
        "id": "StTa1BJQ2kpH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# Utility Functions\n",
        "# ========================\n",
        "\n",
        "def log_memory_usage(label=\"Current memory usage\"):\n",
        "    \"\"\"Log the current memory usage of the process.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "    memory_usage_mb = memory_info.rss / (1024 * 1024)\n",
        "\n",
        "    # Get GPU memory usage if available\n",
        "    gpu_memory_usage = \"N/A\"\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            gpu_memory_allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
        "            gpu_memory_reserved = torch.cuda.memory_reserved() / (1024 * 1024)\n",
        "            gpu_memory_usage = f\"Allocated: {gpu_memory_allocated:.2f} MB, Reserved: {gpu_memory_reserved:.2f} MB\"\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(f\"{label} - RAM: {memory_usage_mb:.2f} MB, GPU: {gpu_memory_usage}\")\n",
        "    return memory_usage_mb\n",
        "\n",
        "\n",
        "def check_system_and_recommend_settings():\n",
        "    \"\"\"\n",
        "    Check system resources and recommend appropriate pipeline settings.\n",
        "\n",
        "    Returns:\n",
        "        dict: Recommended settings\n",
        "    \"\"\"\n",
        "    # Check available memory\n",
        "    available_memory_gb = psutil.virtual_memory().available / (1024 * 1024 * 1024)\n",
        "    print(f\"Available system memory: {available_memory_gb:.1f} GB\")\n",
        "\n",
        "    # Check GPU\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "    gpu_memory_gb = 0\n",
        "    if gpu_available:\n",
        "        try:\n",
        "            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024 * 1024)\n",
        "            print(f\"GPU available: {gpu_available}, with {gpu_memory_gb:.1f} GB memory\")\n",
        "        except:\n",
        "            gpu_memory_gb = 0\n",
        "            print(f\"GPU available but memory info not accessible\")\n",
        "    else:\n",
        "        print(\"GPU not available\")\n",
        "\n",
        "    # Recommend feature set\n",
        "    if available_memory_gb > 40 or (gpu_available and gpu_memory_gb > 20):\n",
        "        feature_set = \"medium\"\n",
        "        sample_size = 200000\n",
        "    elif available_memory_gb > 20 or (gpu_available and gpu_memory_gb > 10):\n",
        "        feature_set = \"medium\"\n",
        "        sample_size = 100000\n",
        "    else:\n",
        "        feature_set = \"small\"\n",
        "        sample_size = 50000\n",
        "\n",
        "    # Recommend model size\n",
        "    if gpu_available and gpu_memory_gb > 16:\n",
        "        # Large model\n",
        "        embed_dim = 256\n",
        "        num_layers = 4\n",
        "        num_heads = 8\n",
        "        batch_size = 64\n",
        "    elif gpu_available and gpu_memory_gb > 8:\n",
        "        # Medium model\n",
        "        embed_dim = 192\n",
        "        num_layers = 3\n",
        "        num_heads = 6\n",
        "        batch_size = 48\n",
        "    else:\n",
        "        # Small model\n",
        "        embed_dim = 128\n",
        "        num_layers = 2\n",
        "        num_heads = 4\n",
        "        batch_size = 32\n",
        "\n",
        "    # Recommendation\n",
        "    settings = {\n",
        "        'feature_set': feature_set,\n",
        "        'sample_size': sample_size,\n",
        "        'embed_dim': embed_dim,\n",
        "        'num_layers': num_layers,\n",
        "        'num_heads': num_heads,\n",
        "        'batch_size': batch_size,\n",
        "        'domains_only': available_memory_gb < 12,\n",
        "        'alphafold_recommended': available_memory_gb > 16 or (gpu_available and gpu_memory_gb > 8)\n",
        "    }\n",
        "\n",
        "    print(\"\\nRecommended settings based on your system:\")\n",
        "    print(f\"Feature set: {settings['feature_set']}\")\n",
        "    print(f\"Sample size: {settings['sample_size']}\")\n",
        "    print(f\"Transformer model: {settings['embed_dim']} dim, {settings['num_layers']} layers, {settings['num_heads']} heads\")\n",
        "    print(f\"Batch size: {settings['batch_size']}\")\n",
        "\n",
        "    if settings['domains_only']:\n",
        "        print(\"RECOMMENDATION: Run domains-only pipeline to avoid memory issues\")\n",
        "\n",
        "    if not settings['alphafold_recommended']:\n",
        "        print(\"RECOMMENDATION: Skip AlphaFold transformer model due to resource constraints\")\n",
        "\n",
        "    return settings\n",
        "\n",
        "\n",
        "def check_existing_pipeline_outputs(output_dir, pipeline_type=\"all\"):\n",
        "    \"\"\"\n",
        "    Check for existing pipeline outputs and provide options to reuse them.\n",
        "    \"\"\"\n",
        "    import glob\n",
        "\n",
        "    results = {\n",
        "        \"exists\": True,\n",
        "        \"domains_data\": False,\n",
        "        \"domain_models\": False,\n",
        "        \"alphafold_model\": False,\n",
        "        \"predictions\": False,\n",
        "        \"reusable\": False,\n",
        "        \"message\": \"\",\n",
        "        \"files\": {}\n",
        "    }\n",
        "\n",
        "    # Check for domain models - could be in 'domain_models' directly\n",
        "    models_dir = os.path.join(output_dir, 'domain_models')  # Updated path\n",
        "    if pipeline_type in [\"domains\", \"all\"] and os.path.exists(models_dir):\n",
        "        model_files = glob.glob(os.path.join(models_dir, \"*_model.joblib\"))\n",
        "        metadata_files = glob.glob(os.path.join(models_dir, \"*_metadata.json\"))\n",
        "\n",
        "        # Check if there are enough model files and metadata files\n",
        "        if model_files and len(model_files) >= 1:\n",
        "            results[\"domain_models\"] = True\n",
        "            results[\"files\"][\"domain_models\"] = models_dir\n",
        "            results[\"reusable\"] = True  # Set to reusable if we found models\n",
        "            results[\"message\"] = \"Found existing domain models\"\n",
        "            print(f\"Found existing domain models: {len(model_files)} models in {models_dir}\")\n",
        "\n",
        "    # Add checks for summary and weight files specifically\n",
        "    ensemble_weights_path = os.path.join('domain_models', 'ensemble_weights.json')\n",
        "    validation_scores_path = os.path.join('domain_models', 'validation_scores.json')\n",
        "\n",
        "    if os.path.exists(ensemble_weights_path):\n",
        "        if \"summary\" not in results[\"files\"]:\n",
        "            results[\"files\"][\"summary\"] = []\n",
        "        results[\"files\"][\"summary\"].append(ensemble_weights_path)\n",
        "        results[\"reusable\"] = True\n",
        "        print(f\"Found ensemble weights: {ensemble_weights_path}\")\n",
        "\n",
        "    if os.path.exists(validation_scores_path):\n",
        "        if \"summary\" not in results[\"files\"]:\n",
        "            results[\"files\"][\"summary\"] = []\n",
        "        results[\"files\"][\"summary\"].append(validation_scores_path)\n",
        "        results[\"reusable\"] = True\n",
        "        print(f\"Found validation scores: {validation_scores_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def reuse_existing_data(output_dir, data_version, feature_set, pipeline_type=\"all\"):\n",
        "    \"\"\"\n",
        "    Set up the pipeline to reuse existing data when available.\n",
        "    \"\"\"\n",
        "    existing_data = check_existing_pipeline_outputs(output_dir, pipeline_type)\n",
        "\n",
        "    if not existing_data[\"reusable\"]:\n",
        "        print(\"No reusable existing data found\")\n",
        "        return {\"reuse\": False}\n",
        "\n",
        "    reuse_config = {\"reuse\": True}\n",
        "\n",
        "    # Ask user if they want to reuse existing data\n",
        "    print(\"\\nExisting pipeline outputs found.\")\n",
        "    reuse = input(\"Do you want to reuse existing data? (y/n): \")\n",
        "\n",
        "    if reuse.lower() != 'y':\n",
        "        print(\"Will not reuse existing data\")\n",
        "        return {\"reuse\": False}\n",
        "\n",
        "    # Check and load domain models\n",
        "    if existing_data[\"domain_models\"] and pipeline_type in [\"domains\", \"all\", \"comprehensive\"]:\n",
        "        models_dir = existing_data[\"files\"][\"domain_models\"]\n",
        "        try:\n",
        "            domain_models = load_domain_models(models_dir)\n",
        "            if domain_models:\n",
        "                reuse_config[\"domain_models\"] = domain_models\n",
        "                print(f\"Will reuse {len(domain_models)} domain models from {models_dir}\")\n",
        "\n",
        "                # If we have models but no domain data, try to recreate feature groups from metadata\n",
        "                if \"domain_data\" not in reuse_config:\n",
        "                    feature_groups = {}\n",
        "                    for domain, model_info in domain_models.items():\n",
        "                        features = model_info.get('features', [])\n",
        "                        if features:\n",
        "                            feature_groups[domain] = features\n",
        "\n",
        "                    if feature_groups:\n",
        "                        reuse_config[\"feature_groups\"] = feature_groups\n",
        "                        print(f\"Reconstructed {len(feature_groups)} feature groups from model metadata\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading domain models: {e}\")\n",
        "\n",
        "    return reuse_config\n"
      ],
      "metadata": {
        "id": "esCefe5O11Qg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# Domain Model Functions\n",
        "# ========================\n",
        "\n",
        "def load_domain_models(models_dir='domain_models'):\n",
        "    \"\"\"\n",
        "    Load all saved domain models and their metadata\n",
        "\n",
        "    Args:\n",
        "        models_dir: Directory where domain models are saved\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of domain models and their metadata\n",
        "    \"\"\"\n",
        "    if not os.path.exists(models_dir):\n",
        "        print(f\"Error: Models directory {models_dir} does not exist\")\n",
        "        return {}\n",
        "\n",
        "    domain_models = {}\n",
        "\n",
        "    # Load ensemble weights if available\n",
        "    ensemble_weights = {}\n",
        "    ensemble_weights_path = os.path.join(models_dir, 'ensemble_weights.json')\n",
        "    if os.path.exists(ensemble_weights_path):\n",
        "        try:\n",
        "            with open(ensemble_weights_path, 'r') as f:\n",
        "                ensemble_data = json.load(f)\n",
        "                ensemble_weights = ensemble_data.get('domains', {})\n",
        "                print(f\"Loaded ensemble weights for {len(ensemble_weights)} domains\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading ensemble weights: {e}\")\n",
        "\n",
        "    # Find all domain model files\n",
        "    model_files = [f for f in os.listdir(models_dir) if f.endswith('_model.joblib')]\n",
        "\n",
        "    for model_file in model_files:\n",
        "        try:\n",
        "            # Extract domain name from filename\n",
        "            domain = model_file.replace('_model.joblib', '')\n",
        "\n",
        "            # Load model\n",
        "            model_path = os.path.join(models_dir, model_file)\n",
        "            model = joblib.load(model_path)\n",
        "\n",
        "            # Load metadata\n",
        "            metadata_path = os.path.join(models_dir, f\"{domain}_metadata.json\")\n",
        "            if os.path.exists(metadata_path):\n",
        "                with open(metadata_path, 'r') as f:\n",
        "                    metadata = json.load(f)\n",
        "            else:\n",
        "                print(f\"Warning: No metadata found for {domain}, using defaults\")\n",
        "                metadata = {\n",
        "                    'domain': domain,\n",
        "                    'features': [],\n",
        "                    'score': 0.0,\n",
        "                    'weight': ensemble_weights.get(domain, 0.0)\n",
        "                }\n",
        "\n",
        "            # Add model to dictionary\n",
        "            domain_models[domain] = {\n",
        "                'model': model,\n",
        "                'features': metadata['features'],\n",
        "                'score': metadata['score'],\n",
        "                'weight': metadata.get('weight', ensemble_weights.get(domain, 0.0))\n",
        "            }\n",
        "            print(f\"Loaded model for {domain} with {len(metadata['features'])} features\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {model_file}: {e}\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(domain_models)} domain models\")\n",
        "    return domain_models\n",
        "\n",
        "\n",
        "def predict_with_domain_models(df, domain_models, target_col=None):\n",
        "    \"\"\"\n",
        "    Make predictions using domain models\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing features\n",
        "        domain_models: Dictionary of domain models from load_domain_models()\n",
        "        target_col: Optional target column for evaluation\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing predictions and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Initialize results\n",
        "    results = {\n",
        "        'domain_predictions': {},\n",
        "        'ensemble_prediction': None,\n",
        "        'domain_scores': {},\n",
        "        'best_domain': None,\n",
        "        'best_score': 0.0\n",
        "    }\n",
        "\n",
        "    if len(domain_models) == 0:\n",
        "        print(\"No domain models available for prediction\")\n",
        "        return results\n",
        "\n",
        "    # For ensemble prediction\n",
        "    ensemble_pred = None\n",
        "    total_weight = 0.0\n",
        "\n",
        "    # Make predictions with each domain model\n",
        "    for domain, model_info in domain_models.items():\n",
        "        model = model_info['model']\n",
        "        features = model_info['features']\n",
        "        weight = model_info['weight']\n",
        "\n",
        "        # Check if all features are available\n",
        "        missing_features = [f for f in features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            print(f\"Warning: {len(missing_features)} features missing for {domain}\")\n",
        "            if len(missing_features) / len(features) > 0.1:  # If more than 10% missing\n",
        "                print(f\"Skipping {domain} due to too many missing features\")\n",
        "                continue\n",
        "\n",
        "        # Prepare features\n",
        "        valid_features = [f for f in features if f in df.columns]\n",
        "        if len(valid_features) < 5:\n",
        "            print(f\"Not enough valid features for {domain}, skipping\")\n",
        "            continue\n",
        "\n",
        "        X = df[valid_features].fillna(0).astype(np.float32)\n",
        "\n",
        "        # Make predictions\n",
        "        try:\n",
        "            domain_pred = model.predict(X)\n",
        "            results['domain_predictions'][domain] = domain_pred\n",
        "\n",
        "            # Add to ensemble if weight > 0\n",
        "            if weight > 0:\n",
        "                if ensemble_pred is None:\n",
        "                    ensemble_pred = np.zeros_like(domain_pred)\n",
        "                ensemble_pred += weight * domain_pred\n",
        "                total_weight += weight\n",
        "\n",
        "            # Evaluate if target column provided\n",
        "            if target_col is not None and target_col in df.columns:\n",
        "                y_true = df[target_col].fillna(0).astype(np.float32).values\n",
        "\n",
        "                # Calculate correlation\n",
        "                mask = ~np.isnan(domain_pred) & ~np.isnan(y_true)\n",
        "                if mask.sum() >= 10:  # Need at least 10 valid pairs\n",
        "                    corr = np.corrcoef(domain_pred[mask], y_true[mask])[0, 1]\n",
        "                    results['domain_scores'][domain] = float(corr)\n",
        "\n",
        "                    # Track best domain\n",
        "                    if corr > results['best_score']:\n",
        "                        results['best_domain'] = domain\n",
        "                        results['best_score'] = float(corr)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error making predictions with {domain}: {e}\")\n",
        "\n",
        "    # Normalize ensemble prediction\n",
        "    if ensemble_pred is not None and total_weight > 0:\n",
        "        ensemble_pred = ensemble_pred / total_weight\n",
        "        results['ensemble_prediction'] = ensemble_pred\n",
        "\n",
        "        # Evaluate ensemble if target provided\n",
        "        if target_col is not None and target_col in df.columns:\n",
        "            y_true = df[target_col].fillna(0).astype(np.float32).values\n",
        "\n",
        "            # Calculate correlation\n",
        "            mask = ~np.isnan(ensemble_pred) & ~np.isnan(y_true)\n",
        "            if mask.sum() >= 10:\n",
        "                corr = np.corrcoef(ensemble_pred[mask], y_true[mask])[0, 1]\n",
        "                results['ensemble_score'] = float(corr)\n",
        "\n",
        "                # Check if ensemble is best\n",
        "                if corr > results['best_score']:\n",
        "                    results['best_model'] = 'ensemble'\n",
        "                    results['best_score'] = float(corr)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Made predictions with {len(results['domain_predictions'])} domain models\")\n",
        "    if 'ensemble_prediction' in results and results['ensemble_prediction'] is not None:\n",
        "        print(f\"Ensemble prediction created with {total_weight:.4f} total weight\")\n",
        "    if 'domain_scores' in results:\n",
        "        print(f\"Domain performance:\")\n",
        "        for domain, score in sorted(results['domain_scores'].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "            print(f\"  {domain}: {score:.4f}\")\n",
        "        if 'ensemble_score' in results:\n",
        "            print(f\"Ensemble score: {results['ensemble_score']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_domain_models(models_dict, feature_groups, output_dir='domain_models'):\n",
        "    \"\"\"\n",
        "    Save trained domain models to disk\n",
        "\n",
        "    Args:\n",
        "        models_dict: Dictionary of trained models\n",
        "        feature_groups: Dictionary of feature groups for each domain\n",
        "        output_dir: Directory to save models\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "    # Save each model\n",
        "    ensemble_weights = {'domains': {}}\n",
        "\n",
        "    for domain, model_info in models_dict.items():\n",
        "        model = model_info.get('model')\n",
        "        if model is None:\n",
        "            print(f\"Warning: No model found for {domain}, skipping\")\n",
        "            continue\n",
        "\n",
        "        score = model_info.get('score', 0.0)\n",
        "        weight = model_info.get('weight', 0.0)\n",
        "\n",
        "        # Get features for this domain\n",
        "        features = feature_groups.get(domain, [])\n",
        "\n",
        "        # Save model\n",
        "        model_path = os.path.join(output_dir, f\"{domain}_model.joblib\")\n",
        "        try:\n",
        "            joblib.dump(model, model_path)\n",
        "\n",
        "            # Save metadata\n",
        "            metadata = {\n",
        "                'domain': domain,\n",
        "                'features': features,\n",
        "                'score': float(score),\n",
        "                'weight': float(weight),\n",
        "                'saved_at': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            }\n",
        "\n",
        "            metadata_path = os.path.join(output_dir, f\"{domain}_metadata.json\")\n",
        "            with open(metadata_path, 'w') as f:\n",
        "                json.dump(metadata, f, indent=4)\n",
        "\n",
        "            # Add to ensemble weights\n",
        "            ensemble_weights['domains'][domain] = float(weight)\n",
        "\n",
        "            print(f\"Saved model and metadata for {domain}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model for {domain}: {e}\")\n",
        "\n",
        "    # Save ensemble weights\n",
        "    ensemble_path = os.path.join(output_dir, 'ensemble_weights.json')\n",
        "    with open(ensemble_path, 'w') as f:\n",
        "        json.dump(ensemble_weights, f, indent=4)\n",
        "\n",
        "    print(f\"Saved ensemble weights for {len(ensemble_weights['domains'])} domains\")"
      ],
      "metadata": {
        "id": "9897je2E1n3W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# Scoring Functions\n",
        "# ========================\n",
        "\n",
        "def score_predictions_with_numerai_tools(predictions_df, target_col='target', era_col='era'):\n",
        "    \"\"\"\n",
        "    Score predictions using numerai-tools if available\n",
        "\n",
        "    Args:\n",
        "        predictions_df: DataFrame with predictions and targets\n",
        "        target_col: Target column name\n",
        "        era_col: Era column name\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with scoring results\n",
        "    \"\"\"\n",
        "    if not NUMERAI_TOOLS_AVAILABLE:\n",
        "        print(\"numerai-tools not available, using basic scoring\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # First, check if the era column exists in the DataFrame\n",
        "        if era_col not in predictions_df.columns:\n",
        "            print(f\"Error: '{era_col}' column not found in predictions DataFrame\")\n",
        "            print(f\"Available columns: {list(predictions_df.columns)}\")\n",
        "            return None\n",
        "\n",
        "        # Check if target_col exists\n",
        "        if target_col not in predictions_df.columns:\n",
        "            print(f\"Error: '{target_col}' column not found in predictions DataFrame\")\n",
        "            print(f\"Available columns: {list(predictions_df.columns)}\")\n",
        "            return None\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Get prediction columns (those starting with 'pred_' or 'prediction_')\n",
        "        pred_cols = [col for col in predictions_df.columns if col.startswith('pred_') or col.startswith('prediction_')]\n",
        "\n",
        "        if not pred_cols:\n",
        "            print(\"No prediction columns found in DataFrame\")\n",
        "            return None\n",
        "\n",
        "        # Create a DataFrame with just the prediction columns for correlation_contribution\n",
        "        preds_df = predictions_df[pred_cols]\n",
        "\n",
        "        # Get meta-model (use first prediction column as meta-model)\n",
        "        meta_model = predictions_df[pred_cols[0]]\n",
        "\n",
        "        # Get targets\n",
        "        targets = predictions_df[target_col]\n",
        "\n",
        "        # Calculate correlation contribution\n",
        "        try:\n",
        "            cc = correlation_contribution(\n",
        "                preds_df,  # Pass DataFrame of predictions\n",
        "                meta_model,  # Pass meta-model Series\n",
        "                targets  # Pass targets Series\n",
        "            )\n",
        "\n",
        "            # Process results for each prediction column\n",
        "            for pred_col in pred_cols:\n",
        "                # Get correlation by era\n",
        "                # Calculate per-era correlation (requires at least 5 data points per era)\n",
        "                pred_by_era = predictions_df.groupby(era_col).apply(\n",
        "                    lambda x: x[pred_col].corr(x[target_col]) if len(x) > 5 else np.nan\n",
        "                ).dropna()\n",
        "\n",
        "                # Calculate metrics\n",
        "                corr_mean = pred_by_era.mean()\n",
        "                corr_std = pred_by_era.std()\n",
        "                sharpe = corr_mean / corr_std if corr_std > 0 else 0\n",
        "\n",
        "                # Store metrics\n",
        "                results[pred_col] = {\n",
        "                    'mean_correlation': corr_mean,\n",
        "                    'std_correlation': corr_std,\n",
        "                    'sharpe_ratio': sharpe,\n",
        "                    'feature_exposure': cc.get(pred_col, 0),\n",
        "                    'correlation_by_era': pred_by_era.to_dict()\n",
        "                }\n",
        "\n",
        "            print(\"Scoring with numerai-tools completed\")\n",
        "            return results\n",
        "\n",
        "        except Exception as inner_e:\n",
        "            # Catch errors specifically in the correlation_contribution calculation\n",
        "            print(f\"Error in correlation_contribution calculation: {inner_e}\")\n",
        "            traceback.print_exc()\n",
        "            raise inner_e\n",
        "\n",
        "    except Exception as e:\n",
        "        # This catches the error shown in the traceback and prints it\n",
        "        print(f\"Error in scoring with numerai-tools: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Fallback to basic correlation scoring\n",
        "        print(\"Falling back to basic correlation scoring...\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def score_models_with_validation(models_dir='domain_models', data_version=\"v5.0\",\n",
        "                                feature_set=\"medium\", sample_size=100000):\n",
        "    \"\"\"\n",
        "    Score saved models using validation data\n",
        "\n",
        "    Args:\n",
        "        models_dir: Directory containing saved models\n",
        "        data_version: Numerai data version\n",
        "        feature_set: Feature set to use\n",
        "        sample_size: Sample size for validation\n",
        "\n",
        "    Returns:\n",
        "        dict: Scoring results\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Scoring Models with Validation Data ===\")\n",
        "\n",
        "    # Load validation data\n",
        "    train_df, val_df, _, _ = load_data(\n",
        "        data_version=data_version,\n",
        "        feature_set=feature_set,\n",
        "        main_target=\"target\",\n",
        "        num_aux_targets=3,\n",
        "        sample_size=sample_size * 2,  # Use more data for validation\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    if val_df is None:\n",
        "        print(\"Failed to load validation data\")\n",
        "        return None\n",
        "\n",
        "    # Verify 'era' column exists\n",
        "    if 'era' not in val_df.columns:\n",
        "        print(\"Warning: 'era' column not found in validation data\")\n",
        "        print(f\"Available columns: {list(val_df.columns)}\")\n",
        "        print(\"Adding dummy 'era' column for compatibility\")\n",
        "        # Add a dummy era column if needed (e.g., all rows in one era)\n",
        "        val_df['era'] = 1\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = make_predictions_pipeline(\n",
        "        val_df,\n",
        "        target_col=\"target\",\n",
        "        models_dir=models_dir\n",
        "    )\n",
        "\n",
        "    if predictions is None:\n",
        "        print(\"Failed to generate predictions\")\n",
        "        return None\n",
        "\n",
        "    # Verify columns in predictions DataFrame\n",
        "    print(f\"Prediction columns: {list(predictions.columns)}\")\n",
        "\n",
        "    # Ensure target column is present\n",
        "    if 'target' not in predictions.columns and 'target' in val_df.columns:\n",
        "        print(\"Adding target column to predictions DataFrame\")\n",
        "        predictions['target'] = val_df['target']\n",
        "\n",
        "    # Make sure era column is present for proper scoring\n",
        "    if 'era' not in predictions.columns:\n",
        "        print(\"Warning: 'era' column missing from predictions, adding from validation data\")\n",
        "        if 'era' in val_df.columns:\n",
        "            predictions['era'] = val_df['era']\n",
        "        else:\n",
        "            print(\"Creating a dummy 'era' column (all rows in one era)\")\n",
        "            predictions['era'] = 1\n",
        "\n",
        "    # Score with numerai-tools if available\n",
        "    if NUMERAI_TOOLS_AVAILABLE:\n",
        "        print(\"Scoring with numerai-tools...\")\n",
        "        try:\n",
        "            # Check that we have both the target and era columns\n",
        "            if 'target' not in predictions.columns:\n",
        "                print(\"Error: 'target' column not found in predictions DataFrame\")\n",
        "                raise ValueError(\"Missing target column\")\n",
        "\n",
        "            if 'era' not in predictions.columns:\n",
        "                print(\"Error: 'era' column not found in predictions DataFrame\")\n",
        "                raise ValueError(\"Missing era column\")\n",
        "\n",
        "            # Get prediction columns\n",
        "            pred_cols = [col for col in predictions.columns\n",
        "                        if col.startswith('pred_') or col.startswith('prediction_')]\n",
        "\n",
        "            if not pred_cols:\n",
        "                print(\"Error: No prediction columns found\")\n",
        "                raise ValueError(\"No prediction columns\")\n",
        "\n",
        "            # Use our fixed scoring function\n",
        "            scoring_results = score_predictions_with_numerai_tools(\n",
        "                predictions_df=predictions,\n",
        "                target_col=\"target\",\n",
        "                era_col=\"era\"\n",
        "            )\n",
        "\n",
        "            if scoring_results:\n",
        "                # Save results to JSON\n",
        "                scores_path = os.path.join(models_dir, 'validation_scores.json')\n",
        "                with open(scores_path, 'w') as f:\n",
        "                    # Convert numpy types to Python native types for JSON\n",
        "                    json_results = {}\n",
        "                    for key, value in scoring_results.items():\n",
        "                        # Skip correlation_by_era which can be large and may contain non-serializable types\n",
        "                        json_results[key] = {\n",
        "                            k: float(v) if isinstance(v, (np.float32, np.float64)) else v\n",
        "                            for k, v in value.items() if k != 'correlation_by_era'\n",
        "                        }\n",
        "\n",
        "                    json.dump(json_results, f, indent=4)\n",
        "\n",
        "                print(f\"Saved validation scores to {scores_path}\")\n",
        "                return scoring_results\n",
        "        except Exception as e:\n",
        "            print(f\"Error in numerai-tools scoring: {e}\")\n",
        "            traceback.print_exc()\n",
        "            print(\"Falling back to basic correlation scoring...\")\n",
        "    else:\n",
        "        print(\"numerai-tools not available, using basic correlation scoring\")\n",
        "\n",
        "    # Basic correlation scoring as fallback\n",
        "    print(\"Using basic correlation scoring...\")\n",
        "\n",
        "    # Check if 'era' exists for grouping\n",
        "    if 'era' in predictions.columns:\n",
        "        eras = predictions['era'].unique()\n",
        "    else:\n",
        "        # Create a single era if 'era' column is missing\n",
        "        eras = [1]\n",
        "        predictions['era'] = 1\n",
        "\n",
        "    # Calculate metrics per era\n",
        "    results = []\n",
        "    for era in eras:\n",
        "        era_data = predictions[predictions['era'] == era]\n",
        "\n",
        "        if len(era_data) == 0:\n",
        "            continue\n",
        "\n",
        "        era_result = {'era': era}\n",
        "\n",
        "        # Calculate metrics for each prediction column\n",
        "        for col in era_data.columns:\n",
        "            if col.startswith('pred_') or col.startswith('prediction_'):\n",
        "                # Calculate correlation with target\n",
        "                if 'target' in era_data.columns:\n",
        "                    # Use pandas correlation method to avoid potential issues with np.corrcoef\n",
        "                    try:\n",
        "                        # Remove NaN values\n",
        "                        valid_mask = ~np.isnan(era_data[col]) & ~np.isnan(era_data['target'])\n",
        "                        if valid_mask.sum() >= 10:  # Need at least 10 valid pairs\n",
        "                            corr = era_data[col][valid_mask].corr(era_data['target'][valid_mask])\n",
        "                            era_result[f\"{col}_corr\"] = corr\n",
        "                    except Exception as calc_err:\n",
        "                        print(f\"Error calculating correlation for {col} in era {era}: {calc_err}\")\n",
        "\n",
        "        results.append(era_result)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary = {}\n",
        "    prediction_cols = [col for col in results_df.columns if col.endswith('_corr')]\n",
        "\n",
        "    for col in prediction_cols:\n",
        "        model_name = col.replace('_corr', '')\n",
        "        # Handle empty or all-NaN columns\n",
        "        if col in results_df.columns and not results_df[col].isna().all():\n",
        "            mean_corr = results_df[col].mean()\n",
        "            std_corr = results_df[col].std()\n",
        "\n",
        "            summary[model_name] = {\n",
        "                'mean_correlation': mean_corr,\n",
        "                'median_correlation': results_df[col].median(),\n",
        "                'std_correlation': std_corr,\n",
        "                'min_correlation': results_df[col].min(),\n",
        "                'max_correlation': results_df[col].max(),\n",
        "                'sharpe_ratio': mean_corr / std_corr if std_corr > 0 else 0,\n",
        "                'positive_eras': (results_df[col] > 0).mean() * 100  # Percentage of positive eras\n",
        "            }\n",
        "\n",
        "    # Save summary to file\n",
        "    summary_df = pd.DataFrame.from_dict(summary, orient='index')\n",
        "    summary_df.to_csv(os.path.join(models_dir, 'backtest_summary.csv'))\n",
        "\n",
        "    # Save detailed era results\n",
        "    results_df.to_csv(os.path.join(models_dir, 'backtest_by_era.csv'), index=False)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nScoring Summary:\")\n",
        "    for model_name, metrics in sorted(summary.items(),\n",
        "                                     key=lambda x: x[1]['mean_correlation'],\n",
        "                                     reverse=True)[:5]:\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  Mean Correlation: {metrics['mean_correlation']:.4f}\")\n",
        "        print(f\"  Sharpe Ratio: {metrics['sharpe_ratio']:.4f}\")\n",
        "\n",
        "    print(f\"Saved basic scoring results to {models_dir}/backtest_summary.csv\")\n",
        "\n",
        "    return {'summary': summary, 'by_era': results_df}"
      ],
      "metadata": {
        "id": "RLp-aPEX1u4I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# Pipeline Functions\n",
        "# ========================\n",
        "\n",
        "def make_predictions_pipeline(new_data, target_col=None, models_dir='domain_models'):\n",
        "    \"\"\"\n",
        "    End-to-end pipeline for making predictions on new data\n",
        "\n",
        "    Args:\n",
        "        new_data: DataFrame with features\n",
        "        target_col: Optional target column for evaluation\n",
        "        models_dir: Directory containing saved domain models\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with predictions\n",
        "    \"\"\"\n",
        "    # Load domain models\n",
        "    domain_models = load_domain_models(models_dir)\n",
        "    if not domain_models:\n",
        "        print(\"No domain models found, cannot make predictions\")\n",
        "        return None\n",
        "\n",
        "    # Make predictions\n",
        "    results = predict_with_domain_models(new_data, domain_models, target_col)\n",
        "\n",
        "    # Create output DataFrame - keep the original index\n",
        "    output_df = pd.DataFrame(index=new_data.index)\n",
        "\n",
        "    # Add individual domain predictions\n",
        "    for domain, preds in results['domain_predictions'].items():\n",
        "        output_df[f'pred_{domain}'] = preds\n",
        "\n",
        "    # Add ensemble prediction\n",
        "    if results['ensemble_prediction'] is not None:\n",
        "        output_df['prediction_ensemble'] = results['ensemble_prediction']\n",
        "\n",
        "    # Add best prediction\n",
        "    if 'best_model' in results:\n",
        "        if results['best_model'] == 'ensemble':\n",
        "            output_df['prediction_best'] = results['ensemble_prediction']\n",
        "        else:\n",
        "            output_df['prediction_best'] = results['domain_predictions'][results['best_domain']]\n",
        "    elif 'best_domain' in results and results['best_domain'] is not None:\n",
        "        output_df['prediction_best'] = results['domain_predictions'][results['best_domain']]\n",
        "\n",
        "    # Copy important columns from the input data\n",
        "    # This is key - we need to preserve era and other metadata columns\n",
        "    important_cols = ['era'] if 'era' in new_data.columns else []\n",
        "    if target_col is not None and target_col in new_data.columns:\n",
        "        important_cols.append(target_col)\n",
        "\n",
        "    # Add any ID columns\n",
        "    if 'id' in new_data.columns:\n",
        "        important_cols.append('id')\n",
        "\n",
        "    # Copy these columns to the output DataFrame\n",
        "    for col in important_cols:\n",
        "        output_df[col] = new_data[col]\n",
        "\n",
        "    # Log available columns for debugging\n",
        "    print(f\"Created prediction DataFrame with {len(output_df.columns)} columns\")\n",
        "    print(f\"Columns: {list(output_df.columns)}\")\n",
        "\n",
        "    return output_df\n",
        "\n",
        "\n",
        "def run_domains_pipeline(data_version=\"v5.0\",\n",
        "                         feature_set=\"medium\",\n",
        "                         sample_size=100000,\n",
        "                         domain_score_threshold=0.05,\n",
        "                         correlation_threshold=0.95,\n",
        "                         tournament_data_path=None,\n",
        "                         models_dir='domain_models'):\n",
        "    \"\"\"\n",
        "    Run the integrated pipeline from domain identification to predictions\n",
        "\n",
        "    Args:\n",
        "        data_version: Numerai data version\n",
        "        feature_set: Feature set to use (small, medium, all)\n",
        "        sample_size: Number of samples to use\n",
        "        domain_score_threshold: Min threshold for domain scores\n",
        "        correlation_threshold: Threshold for feature correlation pruning\n",
        "        tournament_data_path: Path to tournament data file for predictions\n",
        "        models_dir: Directory to save/load models\n",
        "\n",
        "    Returns:\n",
        "        dict: Results from the pipeline\n",
        "    \"\"\"\n",
        "    # Start with clean memory\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    start_time = time.time()\n",
        "    log_memory_usage(\"Starting pipeline\")\n",
        "\n",
        "    results = {\n",
        "        'domains_identified': 0,\n",
        "        'models_trained': 0,\n",
        "        'models_saved': True,\n",
        "        'validation_score': None,\n",
        "        'tournament_predictions': None\n",
        "    }\n",
        "\n",
        "    # Step 1: Run domains pipeline to identify feature groups\n",
        "    print(\"\\n=== Phase 1: Domain Identification ===\")\n",
        "    domains_results = run_domains_only_pipeline(\n",
        "        data_version=data_version,\n",
        "        feature_set=feature_set,\n",
        "        main_target=\"target\",\n",
        "        aux_targets=[\"target_cyrusd_20\", \"target_cyrusd_60\", \"target_teager2b_20\", \"target_teager2b_60\"],\n",
        "        sample_size=sample_size,\n",
        "        n_clusters=None,  # Auto-determine optimal clusters\n",
        "        use_incremental=True,\n",
        "        skip_visualizations=True,\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    if not domains_results or 'feature_groups' not in domains_results:\n",
        "        print(\"Domain identification failed\")\n",
        "        return results\n",
        "\n",
        "    feature_groups = domains_results['feature_groups']\n",
        "    results['domains_identified'] = len(feature_groups)\n",
        "    print(f\"Identified {results['domains_identified']} domains\")\n",
        "\n",
        "    # Log memory usage\n",
        "    log_memory_usage(\"After domain identification\")\n",
        "\n",
        "    # Step 2: Run follow-up pipeline to train models\n",
        "    print(\"\\n=== Phase 2: Model Training ===\")\n",
        "\n",
        "    # Clean memory before loading data\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Load data for training\n",
        "    train_df, val_df, features, all_targets = load_data(\n",
        "        data_version=data_version,\n",
        "        feature_set=feature_set,\n",
        "        main_target=\"target\",\n",
        "        num_aux_targets=3,\n",
        "        sample_size=sample_size,\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    if train_df is None or val_df is None:\n",
        "        print(\"Failed to load training/validation data\")\n",
        "        return results\n",
        "\n",
        "    log_memory_usage(\"After data loading\")\n",
        "\n",
        "    # Run follow-up pipeline\n",
        "    followup_results = follow_up_domains_pipeline(\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        feature_groups=feature_groups,\n",
        "        main_target=\"target\",\n",
        "        domain_score_threshold=domain_score_threshold,\n",
        "        correlation_threshold=correlation_threshold\n",
        "    )\n",
        "\n",
        "    if not followup_results or 'domain_models' not in followup_results:\n",
        "        print(\"Model training failed\")\n",
        "        return results\n",
        "\n",
        "    domain_models = followup_results['domain_models']\n",
        "    results['models_trained'] = len(domain_models)\n",
        "    print(f\"Trained {results['models_trained']} domain models\")\n",
        "\n",
        "    # Save the final score\n",
        "    if 'final_model_score' in followup_results:\n",
        "        results['validation_score'] = followup_results['final_model_score']\n",
        "        print(f\"Validation score: {results['validation_score']:.4f}\")\n",
        "\n",
        "    # Step 3: Save models\n",
        "    print(\"\\n=== Phase 3: Saving Models ===\")\n",
        "    try:\n",
        "        save_domain_models(\n",
        "            models_dict=domain_models,\n",
        "            feature_groups=feature_groups,\n",
        "            output_dir=models_dir\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving models: {e}\")\n",
        "        results['models_saved'] = False\n",
        "\n",
        "    # Step 4: Make tournament predictions if path provided\n",
        "    if tournament_data_path:\n",
        "        print(\"\\n=== Phase 4: Tournament Predictions ===\")\n",
        "        try:\n",
        "            # Clean memory\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Load tournament data\n",
        "            print(f\"Loading tournament data from {tournament_data_path}\")\n",
        "            tournament_data = pd.read_parquet(tournament_data_path)\n",
        "            print(f\"Loaded tournament data with shape {tournament_data.shape}\")\n",
        "\n",
        "            # Convert features to float32\n",
        "            for col in tournament_data.columns:\n",
        "                if col in features:\n",
        "                    tournament_data[col] = tournament_data[col].astype(np.float32)\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = make_predictions_pipeline(\n",
        "                tournament_data,\n",
        "                models_dir=models_dir\n",
        "            )\n",
        "\n",
        "            if predictions is not None:\n",
        "                # Format for submission\n",
        "                if 'prediction_ensemble' in predictions.columns:\n",
        "                    predictions['prediction'] = predictions['prediction_ensemble']\n",
        "                elif 'prediction_best' in predictions.columns:\n",
        "                    predictions['prediction'] = predictions['prediction_best']\n",
        "\n",
        "                # Keep only required columns\n",
        "                id_cols = ['id', 'era'] if 'id' in tournament_data.columns else ['era']\n",
        "                submission = predictions[id_cols + ['prediction']]\n",
        "\n",
        "                # Save submission\n",
        "                submission_path = 'numerai_submission.csv'\n",
        "                submission.to_csv(submission_path, index=False)\n",
        "\n",
        "                print(f\"Saved tournament predictions to {submission_path}\")\n",
        "                results['tournament_predictions'] = submission_path\n",
        "            else:\n",
        "                print(\"Failed to generate predictions\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error making tournament predictions: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # Calculate total runtime\n",
        "    total_time = (time.time() - start_time) / 60  # minutes\n",
        "    print(f\"\\nPipeline completed in {total_time:.2f} minutes\")\n",
        "\n",
        "    # Log final memory usage\n",
        "    log_memory_usage(\"End of pipeline\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def combine_predictions(domains_dir, alphafold_dir, output_path):\n",
        "    \"\"\"Combine predictions from different pipeline components\"\"\"\n",
        "    try:\n",
        "        # Load domain predictions\n",
        "        domain_preds = pd.read_csv(os.path.join(domains_dir, 'numerai_submission.csv'))\n",
        "\n",
        "        # Load AlphaFold predictions\n",
        "        alphafold_preds = pd.read_csv(os.path.join(alphafold_dir, 'predictions.csv'))\n",
        "\n",
        "        # Ensemble predictions\n",
        "        combined = pd.DataFrame({\n",
        "            'id': domain_preds['id'],\n",
        "            'era': domain_preds['era'],\n",
        "            'pred_domains': domain_preds['prediction'],\n",
        "            'pred_alphafold': alphafold_preds['prediction']\n",
        "        })\n",
        "\n",
        "        # Simple average ensemble\n",
        "        combined['prediction'] = combined[['pred_domains', 'pred_alphafold']].mean(axis=1)\n",
        "        combined.to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"Saved final ensemble predictions to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error combining predictions: {e}\")\n",
        "\n",
        "def run_alphafold_pipeline(data_version=\"v5.0\",\n",
        "                          feature_set=\"medium\",\n",
        "                          sample_size=100000,\n",
        "                          main_target=\"target\",\n",
        "                          embed_dim=256,\n",
        "                          num_layers=4,\n",
        "                          num_heads=8,\n",
        "                          batch_size=64,\n",
        "                          epochs=10,\n",
        "                          output_dir=\"alphafold_results\"):\n",
        "    \"\"\"\n",
        "    Run the AlphaFold-inspired pipeline for Numerai.\n",
        "\n",
        "    This includes:\n",
        "    1. NumerAIFold transformer model training\n",
        "    2. Feature embeddings generation\n",
        "    3. Confidence-weighted predictions\n",
        "    4. Evaluation with Numerai-specific metrics\n",
        "\n",
        "    Args:\n",
        "        data_version: Numerai data version\n",
        "        feature_set: Size of feature set\n",
        "        sample_size: Number of samples to use\n",
        "        main_target: Primary target column\n",
        "        embed_dim: Embedding dimension for transformer\n",
        "        num_layers: Number of transformer layers\n",
        "        num_heads: Number of attention heads\n",
        "        batch_size: Batch size for training\n",
        "        epochs: Number of training epochs\n",
        "        output_dir: Directory to save results\n",
        "\n",
        "    Returns:\n",
        "        dict: Results of the pipeline\n",
        "    \"\"\"\n",
        "    from numeraifold.pipeline.execution import run_alphafold_pipeline\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Set random seed\n",
        "    set_seed(RANDOM_SEED)\n",
        "\n",
        "    # Clean memory\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    log_memory_usage(\"Starting AlphaFold pipeline\")\n",
        "\n",
        "    print(\"\\n=== Loading Data ===\")\n",
        "    # Load data\n",
        "    aux_targets = [\"target_cyrusd_20\", \"target_cyrusd_60\", \"target_teager2b_20\", \"target_teager2b_60\"]\n",
        "\n",
        "    train_df, val_df, features, targets = load_data(\n",
        "        data_version=data_version,\n",
        "        feature_set=feature_set,\n",
        "        main_target=main_target,\n",
        "        aux_targets=aux_targets,\n",
        "        sample_size=sample_size,\n",
        "        random_seed=RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    if train_df is None or val_df is None:\n",
        "        print(\"Failed to load data\")\n",
        "        return {\"error\": \"Data loading failed\"}\n",
        "\n",
        "    print(f\"Loaded data: Train shape {train_df.shape}, Val shape {val_df.shape}\")\n",
        "    print(f\"Using {len(features)} features and {len(targets)} targets\")\n",
        "    log_memory_usage(\"After data loading\")\n",
        "\n",
        "    # Run the AlphaFold pipeline\n",
        "    print(\"\\n=== Running AlphaFold Pipeline ===\")\n",
        "    results = run_alphafold_pipeline(\n",
        "        train_df=train_df,\n",
        "        val_df=val_df,\n",
        "        features=features,\n",
        "        targets=targets,\n",
        "        n_clusters=None,  # Auto-determine\n",
        "        confidence_threshold=0.5,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        embed_dim=embed_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        random_seed=RANDOM_SEED,\n",
        "        save_domains=True,\n",
        "        domains_save_path=os.path.join(output_dir, 'alphafold_domains.csv'),\n",
        "        base_path=output_dir,\n",
        "        save_model=True\n",
        "    )\n",
        "\n",
        "    # Calculate runtime\n",
        "    runtime_minutes = (time.time() - start_time) / 60\n",
        "    print(f\"\\nAlphaFold pipeline completed in {runtime_minutes:.2f} minutes\")\n",
        "\n",
        "    # Process and save results\n",
        "    if 'results_standard' in results:\n",
        "        print(\"\\nStandard Prediction Results:\")\n",
        "        for metric, value in results['results_standard'].items():\n",
        "            print(f\"  {metric}: {value:.6f}\")\n",
        "\n",
        "    if 'results_weighted' in results:\n",
        "        print(\"\\nConfidence-Weighted Prediction Results:\")\n",
        "        for metric, value in results['results_weighted'].items():\n",
        "            print(f\"  {metric}: {value:.6f}\")\n",
        "\n",
        "    log_memory_usage(\"End of AlphaFold pipeline\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "v8JpaOx91o3B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# Main Execution\n",
        "# ========================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the Numerai pipeline with a menu-driven interface\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Numerai ML Pipeline\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Default settings from constants\n",
        "    data_version = DATA_VERSION\n",
        "    feature_set = DEFAULT_FEATURE_SET\n",
        "    sample_size = DEFAULT_SAMPLE_SIZE\n",
        "    output_dir = OUTPUT_DIR\n",
        "    models_dir = MODELS_DIR\n",
        "    alphafold_dir = ALPHAFOLD_DIR\n",
        "\n",
        "    # Check system resources and get recommended settings\n",
        "    settings = check_system_and_recommend_settings()\n",
        "\n",
        "    # Update sample size and feature set if recommended\n",
        "    if settings['sample_size'] < sample_size:\n",
        "        print(f\"Adjusting sample size from {sample_size} to {settings['sample_size']} based on system resources\")\n",
        "        sample_size = settings['sample_size']\n",
        "\n",
        "    if settings['feature_set'] != feature_set:\n",
        "        print(f\"Recommended feature set: {settings['feature_set']} (current: {feature_set})\")\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    set_seed(RANDOM_SEED)\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "    # Create output directories if they don't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(alphafold_dir, exist_ok=True)\n",
        "\n",
        "    # Menu system\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"NUMERAI PIPELINE MENU\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"1. Run Domains Pipeline\")\n",
        "        print(\"2. Run AlphaFold Transformer Pipeline\")\n",
        "        print(\"3. Score Existing Models\")\n",
        "        print(\"4. Setup & Configuration\")\n",
        "        print(\"5. Run Entire Pipeline (Domains + AlphaFold)\")\n",
        "        print(\"6. Exit\")\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-5): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            # Domains Pipeline\n",
        "            print(\"\\n=== DOMAINS PIPELINE ===\")\n",
        "            print(\"This pipeline identifies feature domains and trains domain-specific models.\")\n",
        "\n",
        "            # Check for existing pipeline outputs\n",
        "            reuse_config = reuse_existing_data(output_dir, data_version, feature_set, \"domains\")\n",
        "\n",
        "            if reuse_config[\"reuse\"] and \"domain_models\" in reuse_config:\n",
        "                print(\"\\nReusing existing domain models...\")\n",
        "                # Run validation with existing models\n",
        "                score_models_with_validation(\n",
        "                    models_dir=models_dir,\n",
        "                    data_version=data_version,\n",
        "                    feature_set=feature_set,\n",
        "                    sample_size=min(sample_size * 2, 200000)\n",
        "                )\n",
        "            else:\n",
        "                # Get user parameters\n",
        "                threshold = input(f\"Enter domain score threshold (default: 0.05): \") or \"0.05\"\n",
        "                corr_threshold = input(f\"Enter correlation threshold (default: 0.95): \") or \"0.95\"\n",
        "                tournament_path = input(\"Enter tournament data path (optional): \") or TOURNAMENT_DATA_DEFAULT\n",
        "\n",
        "                # Run domains pipeline\n",
        "                run_domains_pipeline(\n",
        "                    data_version=data_version,\n",
        "                    feature_set=feature_set,\n",
        "                    sample_size=sample_size,\n",
        "                    domain_score_threshold=float(threshold),\n",
        "                    correlation_threshold=float(corr_threshold),\n",
        "                    tournament_data_path=tournament_path,\n",
        "                    models_dir=models_dir\n",
        "                )\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            # AlphaFold Pipeline\n",
        "            print(\"\\n=== ALPHAFOLD TRANSFORMER PIPELINE ===\")\n",
        "            print(\"This pipeline trains a transformer model inspired by AlphaFold architecture.\")\n",
        "\n",
        "            # Check for resource constraints\n",
        "            if not settings['alphafold_recommended']:\n",
        "                confirm = input(\"WARNING: Your system may not have enough resources for AlphaFold. Continue anyway? (y/n): \")\n",
        "                if confirm.lower() != 'y':\n",
        "                    print(\"Operation cancelled\")\n",
        "                    continue\n",
        "\n",
        "            # Get user parameters\n",
        "            embed_dim = input(f\"Enter embedding dimension (default: {settings['embed_dim']}): \") or settings['embed_dim']\n",
        "            num_layers = input(f\"Enter number of layers (default: {settings['num_layers']}): \") or settings['num_layers']\n",
        "            num_heads = input(f\"Enter number of attention heads (default: {settings['num_heads']}): \") or settings['num_heads']\n",
        "            batch_size = input(f\"Enter batch size (default: {settings['batch_size']}): \") or settings['batch_size']\n",
        "            epochs = input(\"Enter number of epochs (default: 10): \") or \"10\"\n",
        "\n",
        "            # Run AlphaFold pipeline\n",
        "            run_alphafold_pipeline(\n",
        "                data_version=data_version,\n",
        "                feature_set=feature_set,\n",
        "                sample_size=sample_size,\n",
        "                embed_dim=int(embed_dim),\n",
        "                num_layers=int(num_layers),\n",
        "                num_heads=int(num_heads),\n",
        "                batch_size=int(batch_size),\n",
        "                epochs=int(epochs),\n",
        "                output_dir=alphafold_dir\n",
        "            )\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            # Score Existing Models\n",
        "            print(\"\\n=== SCORE EXISTING MODELS ===\")\n",
        "\n",
        "            # Check if models exist\n",
        "            if not os.path.exists(models_dir):\n",
        "                print(f\"Error: No models found in {models_dir}\")\n",
        "                continue\n",
        "\n",
        "            # Get sample size for validation\n",
        "            sample_size_input = input(f\"Enter validation sample size (default: {min(sample_size * 2, 200000)}): \") or min(sample_size * 2, 200000)\n",
        "\n",
        "            # Score models\n",
        "            score_models_with_validation(\n",
        "                models_dir=models_dir,\n",
        "                data_version=data_version,\n",
        "                feature_set=feature_set,\n",
        "                sample_size=int(sample_size_input)\n",
        "            )\n",
        "\n",
        "        elif choice == \"4\":\n",
        "            # Setup & Configuration\n",
        "            print(\"\\n=== SETUP & CONFIGURATION ===\")\n",
        "\n",
        "            # Data version\n",
        "            data_version = input(f\"Enter data version (default: {data_version}): \") or data_version\n",
        "\n",
        "            # Feature set\n",
        "            print(\"\\nFeature set options:\")\n",
        "            print(\"1. small  - Lower memory usage, fewer features\")\n",
        "            print(\"2. medium - Balanced performance and memory usage\")\n",
        "            print(\"3. all    - All features, high memory usage\")\n",
        "\n",
        "            feature_choice = input(f\"Choose feature set (default: {feature_set}): \")\n",
        "            if feature_choice == \"1\":\n",
        "                feature_set = \"small\"\n",
        "            elif feature_choice == \"2\":\n",
        "                feature_set = \"medium\"\n",
        "            elif feature_choice == \"3\":\n",
        "                feature_set = \"all\"\n",
        "\n",
        "            # Sample size\n",
        "            sample_size = int(input(f\"Enter sample size (default: {sample_size}): \") or sample_size)\n",
        "\n",
        "            # Output directories\n",
        "            output_dir = input(f\"Enter main output directory (default: {output_dir}): \") or output_dir\n",
        "            models_dir = input(f\"Enter models directory (default: {models_dir}): \") or models_dir\n",
        "            alphafold_dir = input(f\"Enter AlphaFold directory (default: {alphafold_dir}): \") or alphafold_dir\n",
        "\n",
        "            # Create directories if they don't exist\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            os.makedirs(models_dir, exist_ok=True)\n",
        "            os.makedirs(alphafold_dir, exist_ok=True)\n",
        "\n",
        "            print(\"\\nConfiguration updated:\")\n",
        "            print(f\"Data Version: {data_version}\")\n",
        "            print(f\"Feature Set: {feature_set}\")\n",
        "            print(f\"Sample Size: {sample_size}\")\n",
        "            print(f\"Output Directory: {output_dir}\")\n",
        "            print(f\"Models Directory: {models_dir}\")\n",
        "            print(f\"AlphaFold Directory: {alphafold_dir}\")\n",
        "\n",
        "        elif choice == \"5\":\n",
        "            print(\"\\n=== COMPREHENSIVE PIPELINE ===\")\n",
        "            print(\"Running full pipeline including:\")\n",
        "            print(\"- Domain identification and modeling\")\n",
        "            print(\"- AlphaFold transformer training\")\n",
        "            print(\"- Ensemble predictions\\n\")\n",
        "\n",
        "            # Run domains pipeline\n",
        "            domains_results = run_domains_pipeline(\n",
        "                data_version=data_version,\n",
        "                feature_set=feature_set,\n",
        "                sample_size=sample_size,\n",
        "                models_dir=models_dir\n",
        "            )\n",
        "\n",
        "            if domains_results.get('models_trained', 0) > 0:\n",
        "                # Run AlphaFold pipeline\n",
        "                alphafold_results = run_alphafold_pipeline(\n",
        "                    data_version=data_version,\n",
        "                    feature_set=feature_set,\n",
        "                    sample_size=sample_size,\n",
        "                    output_dir=alphafold_dir,\n",
        "                    embed_dim=settings['embed_dim'],\n",
        "                    num_layers=settings['num_layers'],\n",
        "                    num_heads=settings['num_heads'],\n",
        "                    batch_size=settings['batch_size']\n",
        "                )\n",
        "\n",
        "                # Combine predictions\n",
        "                combine_predictions(\n",
        "                    domains_dir=models_dir,\n",
        "                    alphafold_dir=alphafold_dir,\n",
        "                    output_path=os.path.join(output_dir, 'final_predictions.csv')\n",
        "                )\n",
        "\n",
        "        elif choice == \"6\":\n",
        "            # Exit\n",
        "            print(\"\\nExiting Numerai Pipeline. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"\\nInvalid choice. Please enter a number from 1-5.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYFrvyXR2LnZ",
        "outputId": "bdaf6629-3792-4539-f928-6e4a99e16692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Numerai ML Pipeline\n",
            "================================================================================\n",
            "Available system memory: 48.9 GB\n",
            "GPU not available\n",
            "\n",
            "Recommended settings based on your system:\n",
            "Feature set: medium\n",
            "Sample size: 200000\n",
            "Transformer model: 128 dim, 2 layers, 4 heads\n",
            "Batch size: 32\n",
            "CUDA available: False\n",
            "\n",
            "==================================================\n",
            "NUMERAI PIPELINE MENU\n",
            "==================================================\n",
            "1. Run Domains Pipeline\n",
            "2. Run AlphaFold Transformer Pipeline\n",
            "3. Score Existing Models\n",
            "4. Setup & Configuration\n",
            "5. Exit\n",
            "\n",
            "Enter your choice (1-5): 2\n",
            "\n",
            "=== ALPHAFOLD TRANSFORMER PIPELINE ===\n",
            "This pipeline trains a transformer model inspired by AlphaFold architecture.\n",
            "Enter embedding dimension (default: 128): \n",
            "Enter number of layers (default: 2): \n",
            "Enter number of attention heads (default: 4): \n",
            "Enter batch size (default: 32): \n",
            "Enter number of epochs (default: 10): \n",
            "Starting AlphaFold pipeline - RAM: 1349.41 MB, GPU: N/A\n",
            "\n",
            "=== Loading Data ===\n",
            "Loading v5.0 data with medium feature set...\n",
            "Using targets: ['target', 'target_cyrusd_20', 'target_cyrusd_60', 'target_teager2b_20', 'target_teager2b_60']\n",
            "Reading train data with 705 features and 5 targets...\n",
            "A sample size of 100000 has been specified\n",
            "Train file has 2746270 total rows\n",
            "Loading a sample of 100000 rows from training data...\n",
            "Reading validation data...\n",
            "Converting features and targets to float32...\n",
            "Train shape: (100000, 711), Validation shape: (3596488, 711)\n",
            "Memory usage - Train: 276.66 MB, Val: 10200.46 MB\n",
            "Loaded data: Train shape (100000, 711), Val shape (3596488, 711)\n",
            "Using 705 features and 5 targets\n",
            "After data loading - RAM: 15299.93 MB, GPU: N/A\n",
            "\n",
            "=== Running AlphaFold Pipeline ===\n",
            "Starting AlphaFold-inspired Numerai pipeline...\n",
            "----- Phase 1: Feature Domain Identification -----\n",
            "Running Phase 1: Feature Domain Identification\n",
            "Identifying feature domains...\n",
            "Error in domain identification: '<' not supported between instances of 'int' and 'NoneType'\n",
            "Feature domains data saved to numeraifold_results/alphafold/alphafold_domains.csv\n",
            "Domain mapping saved to numeraifold_results/alphafold/alphafold_domains_mapping.csv\n",
            "Feature domain data saved to: numeraifold_results/alphafold/alphafold_domains.csv\n",
            "Too many features (705), limiting labels to 100\n",
            "Calculating feature stability across eras...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing features: 100%|██████████| 705/705 [01:59<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating sequence representation (this may take a while)...\n",
            "Using 20000 samples for sequence representation\n",
            "Creating sequence representation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing eras: 100%|██████████| 574/574 [00:50<00:00, 11.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping evolutionary profiles generation for large dataset\n",
            "Running follow-up domains pipeline to refine and prune features...\n",
            "Starting follow-up domains pipeline with improved NaN handling...\n",
            "Cleaning data and ensuring proper types...\n",
            "Found 705/705 valid features in training data\n",
            "705/705 features were already float32\n",
            "Evaluating domain performance with robust correlation calculation...\n",
            "  domain_0: 0.0038 (from 705 valid features)\n",
            "Domain performance scores (correlation with target):\n",
            "  domain_0: 0.0038\n",
            "\n",
            "Filtering domains with correlation >= 0.51...\n",
            "Kept domains: 0/1\n",
            "Warning: No domains met threshold. Keeping top 3 domains instead.\n",
            "Top domains selected: ['domain_0']\n",
            "Total features from kept domains: 705/705 valid features\n",
            "\n",
            "Pruning correlated features (threshold=0.95)...\n",
            "Using robust correlation pruning approach...\n",
            "Calculating feature-target correlations...\n",
            "Pruning 705 features...\n",
            "Pruning complete. Kept 679 features out of 705\n",
            "\n",
            "Training domain-specific models...\n",
            "Pruning features for domain domain_0...\n"
          ]
        }
      ]
    }
  ]
}